{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtmwam5bo8WHefYKmX0yOC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/NLP-2024-2025/blob/main/Lecture_3_2_%D0%9F%D0%BE%D0%B8%D1%81%D0%BA_%D1%81%D0%BC%D1%8B%D1%81%D0%BB%D0%B0_%D1%81%D0%BB%D0%BE%D0%B2_%D0%BF%D0%BE_%D0%B8%D1%85_%D1%87%D0%B0%D1%81%D1%82%D0%BE%D1%82%D0%BD%D0%BE%D1%81%D1%82%D1%8F%D0%BC_%D1%81%D0%B5%D0%BC%D0%B0%D0%BD%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx7tjSK_8goO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Поиск смысла слов по их частотностям: семантический анализ\n",
        "\n",
        "Введение в семантический анализ и векторное представление слов является основой для разработки современных систем обработки естественного языка (NLP). Эта лекция охватывает несколько ключевых аспектов семантического анализа, включая создание векторов тем, семантический поиск, масштабируемый анализ для больших корпусов текстов и использование семантических компонентов в NLP. Особое внимание будет уделено ориентации в векторных пространствах высокой размерности.\n",
        "\n",
        "#### 1. Создание векторов тем с помощью анализа семантики (смысла)\n",
        "\n",
        "**1.1 Основы семантического анализа**\n",
        "\n",
        "Семантический анализ направлен на выявление смысла слов и предложений путем изучения их контекста. Одним из основных методов является векторное представление слов, также известное как word embeddings. Такие методы, как Word2Vec, GloVe и FastText, используют статистические модели для представления слов в виде векторов в высокоразмерном пространстве, где семантически схожие слова располагаются ближе друг к другу.\n",
        "\n",
        "**1.2 Модели для создания векторов тем**\n",
        "\n",
        "1. **Word2Vec**: Использует модели CBOW и Skip-Gram для обучения векторных представлений слов на больших корпусах текста. CBOW предсказывает слово по контексту, а Skip-Gram делает обратное.\n",
        "2. **GloVe (Global Vectors for Word Representation)**: Основан на матрицах соотношений слов и глобальной статистике корпуса.\n",
        "3. **FastText**: Улучшение Word2Vec, учитывающее морфологическую структуру слов за счет использования субслов.\n",
        "\n"
      ],
      "metadata": {
        "id": "dQkpEoM1-l-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Рассмотрим каждую модели\n",
        "\n",
        "Word2Vec — алгоритм, предлагающий два основных подхода: CBOW (Continuous Bag of Words) и Skip-Gram. Мы рассмотрим оба подхода с соответствующими формулами и примерами.\n",
        "\n",
        "**1.1 CBOW (Continuous Bag of Words)**\n",
        "\n",
        "CBOW предсказывает текущее слово по его контексту.\n",
        "\n",
        "**Целевая функция**:\n",
        "$$ J(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\log P(w_t | w_{t-i}, \\ldots, w_{t+i}) $$\n",
        "\n",
        "**Вероятность**:\n",
        "$$ P(w_t | w_{t-i}, \\ldots, w_{t+i}) = \\frac{\\exp(v_{w_t} \\cdot h)}{\\sum_{w' \\in V} \\exp(v_{w'} \\cdot h)} $$\n",
        "\n",
        "где $ h $ — средний вектор контекстных слов:\n",
        "$$ h = \\frac{1}{2i} \\sum_{j=-i, j \\neq 0}^{i} v_{w_{t+j}} $$\n",
        "\n",
        "**Пример**:\n",
        "Для предложения \"I love natural language processing\", если текущее слово —\n",
        "\n",
        "**Пример предложения**: \"I love natural language processing\".\n",
        "\n",
        "**Контекстное окно**: 2 (по 2 слова слева и справа от целевого слова).\n",
        "\n",
        "**Целевое слово**: \"language\".\n",
        "\n",
        "**Контекстные слова**: [\"I\", \"love\", \"natural\", \"processing\"].\n",
        "\n",
        "**Целевая функция**:\n",
        "$$ J(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\log P(w_t | w_{t-i}, \\ldots, w_{t+i}) $$\n",
        "\n",
        "Для нашего примера:\n",
        "\n",
        "$$ J(\\theta) = \\log P(language | I, love, natural, processing) $$\n",
        "\n",
        "**Вероятность**:\n",
        "$$ P(w_t | w_{t-i}, \\ldots, w_{t+i}) = \\frac{\\exp(v_{w_t} \\cdot h)}{\\sum_{w' \\in V} \\exp(v_{w'} \\cdot h)} $$\n",
        "\n",
        "где $ h $ — средний вектор контекстных слов:\n",
        "\n",
        "$$h = \\frac{1}{4} (v_{I} + v_{love} + v_{natural} + v_{processing}) $$\n",
        "\n",
        "**Шаг 1: Вычисление среднего вектора контекста**\n",
        "\n",
        "Пусть векторы слов имеют размерность 3 для простоты. Допустим, что векторы слов такие:\n",
        "\n",
        "- $ v_{I} = [0.2, 0.1, -0.1] $\n",
        "- $ v_{love} = [0.4, 0.3, 0.2] $\n",
        "- $ v_{natural} = [0.3, 0.4, 0.5] $\n",
        "- $ v_{processing} = [0.5, 0.6, 0.7] $\n",
        "\n",
        "Тогда средний вектор контекста:\n",
        "\n",
        "$$ h = \\frac{1}{4} ([0.2, 0.1, -0.1] + [0.4, 0.3, 0.2] + [0.3, 0.4, 0.5] + [0.5, 0.6, 0.7]) $$\n",
        "$$ h = \\frac{1}{4} ([1.4, 1.4, 1.3]) $$\n",
        "$$ h = [0.35, 0.35, 0.325] $$\n",
        "\n",
        "**Шаг 2: Вычисление вероятности целевого слова**\n",
        "\n",
        "Пусть вектор целевого слова $ v_{language} = [0.6, 0.7, 0.8] $.\n",
        "\n",
        "Скалярное произведение:\n",
        "\n",
        "$$ v_{language} \\cdot h = [0.6, 0.7, 0.8] \\cdot [0.35, 0.35, 0.325] $$\n",
        "$$ v_{language} \\cdot h = 0.6 \\cdot 0.35 + 0.7 \\cdot 0.35 + 0.8 \\cdot 0.325 $$\n",
        "$$ v_{language} \\cdot h = 0.21 + 0.245 + 0.26 $$\n",
        "$$ v_{language} \\cdot h = 0.715 $$\n",
        "\n",
        "Теперь вероятность:\n",
        "\n",
        "$$ P(language | I, love, natural, processing) = \\frac{\\exp(0.715)}{\\sum_{w' \\in V} \\exp(v_{w'} \\cdot h)} $$\n",
        "\n",
        "Для простоты предположим, что сумма экспонент для всех слов в словаре равна $ Z $:\n",
        "\n",
        "$$ P(language | I, love, natural, processing) = \\frac{\\exp(0.715)}{Z} $$\n",
        "\n",
        "Целевая функция:\n",
        "\n",
        "$$ J(\\theta) = \\log P(language | I, love, natural, processing) = 0.715 - \\log Z $$\n",
        "\n"
      ],
      "metadata": {
        "id": "C0Wby_jICpwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Пример предложения\n",
        "sentences = [\"I love natural language processing\"]\n",
        "\n",
        "# Параметры модели\n",
        "window_size = 2\n",
        "embedding_dim = 3\n",
        "epochs = 100\n",
        "\n",
        "# Токенизация и создание словаря\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "# Функция для создания данных для CBOW\n",
        "def generate_cbow_data(sequences, window_size):\n",
        "    data = []\n",
        "    labels = []\n",
        "    for sequence in sequences:\n",
        "        for i, word_id in enumerate(sequence):\n",
        "            context = []\n",
        "            for j in range(-window_size, window_size + 1):\n",
        "                if j != 0 and 0 <= i + j < len(sequence):\n",
        "                    context.append(sequence[i + j])\n",
        "            if len(context) == 2 * window_size:\n",
        "                data.append(context)\n",
        "                labels.append(word_id)\n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "# Создание данных для обучения\n",
        "data, labels = generate_cbow_data(sequences, window_size)\n",
        "\n",
        "# Модель CBOW\n",
        "input_layer = tf.keras.layers.Input(shape=(2 * window_size,), name='input')\n",
        "embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=2 * window_size, name='embedding')(input_layer)\n",
        "context_layer = tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=1), name='context')(embedding_layer)\n",
        "output_layer = tf.keras.layers.Dense(vocab_size, activation='softmax', name='output')(context_layer)\n",
        "\n",
        "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "model.summary()\n",
        "\n",
        "# Обучение модели\n",
        "model.fit(data, labels, epochs=epochs, verbose=2)\n",
        "\n",
        "# Получение векторов слов\n",
        "word_embeddings = model.get_layer('embedding').get_weights()[0]\n",
        "print(\"Word Embeddings:\", word_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwMxM-aSEvAm",
        "outputId": "0fc9e7dc-9c96-4000-996e-3db20f0d9ab5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input (InputLayer)          [(None, 4)]               0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 4, 3)              18        \n",
            "                                                                 \n",
            " context (Lambda)            (None, 3)                 0         \n",
            "                                                                 \n",
            " output (Dense)              (None, 6)                 24        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 42 (168.00 Byte)\n",
            "Trainable params: 42 (168.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "1/1 - 0s - loss: 1.7819 - 479ms/epoch - 479ms/step\n",
            "Epoch 2/100\n",
            "1/1 - 0s - loss: 1.7793 - 8ms/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1/1 - 0s - loss: 1.7767 - 8ms/epoch - 8ms/step\n",
            "Epoch 4/100\n",
            "1/1 - 0s - loss: 1.7741 - 6ms/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1/1 - 0s - loss: 1.7715 - 8ms/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "1/1 - 0s - loss: 1.7689 - 8ms/epoch - 8ms/step\n",
            "Epoch 7/100\n",
            "1/1 - 0s - loss: 1.7663 - 8ms/epoch - 8ms/step\n",
            "Epoch 8/100\n",
            "1/1 - 0s - loss: 1.7637 - 8ms/epoch - 8ms/step\n",
            "Epoch 9/100\n",
            "1/1 - 0s - loss: 1.7610 - 8ms/epoch - 8ms/step\n",
            "Epoch 10/100\n",
            "1/1 - 0s - loss: 1.7584 - 8ms/epoch - 8ms/step\n",
            "Epoch 11/100\n",
            "1/1 - 0s - loss: 1.7558 - 10ms/epoch - 10ms/step\n",
            "Epoch 12/100\n",
            "1/1 - 0s - loss: 1.7531 - 7ms/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "1/1 - 0s - loss: 1.7504 - 8ms/epoch - 8ms/step\n",
            "Epoch 14/100\n",
            "1/1 - 0s - loss: 1.7477 - 7ms/epoch - 7ms/step\n",
            "Epoch 15/100\n",
            "1/1 - 0s - loss: 1.7451 - 9ms/epoch - 9ms/step\n",
            "Epoch 16/100\n",
            "1/1 - 0s - loss: 1.7424 - 7ms/epoch - 7ms/step\n",
            "Epoch 17/100\n",
            "1/1 - 0s - loss: 1.7396 - 10ms/epoch - 10ms/step\n",
            "Epoch 18/100\n",
            "1/1 - 0s - loss: 1.7369 - 8ms/epoch - 8ms/step\n",
            "Epoch 19/100\n",
            "1/1 - 0s - loss: 1.7342 - 9ms/epoch - 9ms/step\n",
            "Epoch 20/100\n",
            "1/1 - 0s - loss: 1.7315 - 8ms/epoch - 8ms/step\n",
            "Epoch 21/100\n",
            "1/1 - 0s - loss: 1.7287 - 8ms/epoch - 8ms/step\n",
            "Epoch 22/100\n",
            "1/1 - 0s - loss: 1.7259 - 8ms/epoch - 8ms/step\n",
            "Epoch 23/100\n",
            "1/1 - 0s - loss: 1.7231 - 7ms/epoch - 7ms/step\n",
            "Epoch 24/100\n",
            "1/1 - 0s - loss: 1.7204 - 7ms/epoch - 7ms/step\n",
            "Epoch 25/100\n",
            "1/1 - 0s - loss: 1.7175 - 8ms/epoch - 8ms/step\n",
            "Epoch 26/100\n",
            "1/1 - 0s - loss: 1.7147 - 8ms/epoch - 8ms/step\n",
            "Epoch 27/100\n",
            "1/1 - 0s - loss: 1.7119 - 8ms/epoch - 8ms/step\n",
            "Epoch 28/100\n",
            "1/1 - 0s - loss: 1.7091 - 8ms/epoch - 8ms/step\n",
            "Epoch 29/100\n",
            "1/1 - 0s - loss: 1.7062 - 8ms/epoch - 8ms/step\n",
            "Epoch 30/100\n",
            "1/1 - 0s - loss: 1.7033 - 7ms/epoch - 7ms/step\n",
            "Epoch 31/100\n",
            "1/1 - 0s - loss: 1.7004 - 8ms/epoch - 8ms/step\n",
            "Epoch 32/100\n",
            "1/1 - 0s - loss: 1.6975 - 7ms/epoch - 7ms/step\n",
            "Epoch 33/100\n",
            "1/1 - 0s - loss: 1.6946 - 8ms/epoch - 8ms/step\n",
            "Epoch 34/100\n",
            "1/1 - 0s - loss: 1.6917 - 11ms/epoch - 11ms/step\n",
            "Epoch 35/100\n",
            "1/1 - 0s - loss: 1.6888 - 8ms/epoch - 8ms/step\n",
            "Epoch 36/100\n",
            "1/1 - 0s - loss: 1.6858 - 7ms/epoch - 7ms/step\n",
            "Epoch 37/100\n",
            "1/1 - 0s - loss: 1.6828 - 7ms/epoch - 7ms/step\n",
            "Epoch 38/100\n",
            "1/1 - 0s - loss: 1.6799 - 7ms/epoch - 7ms/step\n",
            "Epoch 39/100\n",
            "1/1 - 0s - loss: 1.6769 - 10ms/epoch - 10ms/step\n",
            "Epoch 40/100\n",
            "1/1 - 0s - loss: 1.6738 - 7ms/epoch - 7ms/step\n",
            "Epoch 41/100\n",
            "1/1 - 0s - loss: 1.6708 - 9ms/epoch - 9ms/step\n",
            "Epoch 42/100\n",
            "1/1 - 0s - loss: 1.6678 - 12ms/epoch - 12ms/step\n",
            "Epoch 43/100\n",
            "1/1 - 0s - loss: 1.6647 - 9ms/epoch - 9ms/step\n",
            "Epoch 44/100\n",
            "1/1 - 0s - loss: 1.6616 - 9ms/epoch - 9ms/step\n",
            "Epoch 45/100\n",
            "1/1 - 0s - loss: 1.6585 - 9ms/epoch - 9ms/step\n",
            "Epoch 46/100\n",
            "1/1 - 0s - loss: 1.6554 - 8ms/epoch - 8ms/step\n",
            "Epoch 47/100\n",
            "1/1 - 0s - loss: 1.6523 - 9ms/epoch - 9ms/step\n",
            "Epoch 48/100\n",
            "1/1 - 0s - loss: 1.6492 - 9ms/epoch - 9ms/step\n",
            "Epoch 49/100\n",
            "1/1 - 0s - loss: 1.6460 - 9ms/epoch - 9ms/step\n",
            "Epoch 50/100\n",
            "1/1 - 0s - loss: 1.6429 - 10ms/epoch - 10ms/step\n",
            "Epoch 51/100\n",
            "1/1 - 0s - loss: 1.6397 - 9ms/epoch - 9ms/step\n",
            "Epoch 52/100\n",
            "1/1 - 0s - loss: 1.6365 - 8ms/epoch - 8ms/step\n",
            "Epoch 53/100\n",
            "1/1 - 0s - loss: 1.6333 - 9ms/epoch - 9ms/step\n",
            "Epoch 54/100\n",
            "1/1 - 0s - loss: 1.6300 - 9ms/epoch - 9ms/step\n",
            "Epoch 55/100\n",
            "1/1 - 0s - loss: 1.6268 - 7ms/epoch - 7ms/step\n",
            "Epoch 56/100\n",
            "1/1 - 0s - loss: 1.6235 - 8ms/epoch - 8ms/step\n",
            "Epoch 57/100\n",
            "1/1 - 0s - loss: 1.6202 - 7ms/epoch - 7ms/step\n",
            "Epoch 58/100\n",
            "1/1 - 0s - loss: 1.6169 - 8ms/epoch - 8ms/step\n",
            "Epoch 59/100\n",
            "1/1 - 0s - loss: 1.6136 - 7ms/epoch - 7ms/step\n",
            "Epoch 60/100\n",
            "1/1 - 0s - loss: 1.6103 - 6ms/epoch - 6ms/step\n",
            "Epoch 61/100\n",
            "1/1 - 0s - loss: 1.6070 - 10ms/epoch - 10ms/step\n",
            "Epoch 62/100\n",
            "1/1 - 0s - loss: 1.6036 - 9ms/epoch - 9ms/step\n",
            "Epoch 63/100\n",
            "1/1 - 0s - loss: 1.6002 - 11ms/epoch - 11ms/step\n",
            "Epoch 64/100\n",
            "1/1 - 0s - loss: 1.5968 - 8ms/epoch - 8ms/step\n",
            "Epoch 65/100\n",
            "1/1 - 0s - loss: 1.5934 - 8ms/epoch - 8ms/step\n",
            "Epoch 66/100\n",
            "1/1 - 0s - loss: 1.5900 - 8ms/epoch - 8ms/step\n",
            "Epoch 67/100\n",
            "1/1 - 0s - loss: 1.5866 - 8ms/epoch - 8ms/step\n",
            "Epoch 68/100\n",
            "1/1 - 0s - loss: 1.5831 - 8ms/epoch - 8ms/step\n",
            "Epoch 69/100\n",
            "1/1 - 0s - loss: 1.5796 - 8ms/epoch - 8ms/step\n",
            "Epoch 70/100\n",
            "1/1 - 0s - loss: 1.5761 - 8ms/epoch - 8ms/step\n",
            "Epoch 71/100\n",
            "1/1 - 0s - loss: 1.5726 - 7ms/epoch - 7ms/step\n",
            "Epoch 72/100\n",
            "1/1 - 0s - loss: 1.5691 - 7ms/epoch - 7ms/step\n",
            "Epoch 73/100\n",
            "1/1 - 0s - loss: 1.5656 - 8ms/epoch - 8ms/step\n",
            "Epoch 74/100\n",
            "1/1 - 0s - loss: 1.5620 - 7ms/epoch - 7ms/step\n",
            "Epoch 75/100\n",
            "1/1 - 0s - loss: 1.5585 - 9ms/epoch - 9ms/step\n",
            "Epoch 76/100\n",
            "1/1 - 0s - loss: 1.5549 - 12ms/epoch - 12ms/step\n",
            "Epoch 77/100\n",
            "1/1 - 0s - loss: 1.5513 - 9ms/epoch - 9ms/step\n",
            "Epoch 78/100\n",
            "1/1 - 0s - loss: 1.5477 - 8ms/epoch - 8ms/step\n",
            "Epoch 79/100\n",
            "1/1 - 0s - loss: 1.5440 - 8ms/epoch - 8ms/step\n",
            "Epoch 80/100\n",
            "1/1 - 0s - loss: 1.5404 - 8ms/epoch - 8ms/step\n",
            "Epoch 81/100\n",
            "1/1 - 0s - loss: 1.5367 - 8ms/epoch - 8ms/step\n",
            "Epoch 82/100\n",
            "1/1 - 0s - loss: 1.5331 - 9ms/epoch - 9ms/step\n",
            "Epoch 83/100\n",
            "1/1 - 0s - loss: 1.5294 - 7ms/epoch - 7ms/step\n",
            "Epoch 84/100\n",
            "1/1 - 0s - loss: 1.5257 - 12ms/epoch - 12ms/step\n",
            "Epoch 85/100\n",
            "1/1 - 0s - loss: 1.5219 - 12ms/epoch - 12ms/step\n",
            "Epoch 86/100\n",
            "1/1 - 0s - loss: 1.5182 - 10ms/epoch - 10ms/step\n",
            "Epoch 87/100\n",
            "1/1 - 0s - loss: 1.5144 - 10ms/epoch - 10ms/step\n",
            "Epoch 88/100\n",
            "1/1 - 0s - loss: 1.5107 - 9ms/epoch - 9ms/step\n",
            "Epoch 89/100\n",
            "1/1 - 0s - loss: 1.5069 - 8ms/epoch - 8ms/step\n",
            "Epoch 90/100\n",
            "1/1 - 0s - loss: 1.5031 - 12ms/epoch - 12ms/step\n",
            "Epoch 91/100\n",
            "1/1 - 0s - loss: 1.4993 - 9ms/epoch - 9ms/step\n",
            "Epoch 92/100\n",
            "1/1 - 0s - loss: 1.4954 - 10ms/epoch - 10ms/step\n",
            "Epoch 93/100\n",
            "1/1 - 0s - loss: 1.4916 - 12ms/epoch - 12ms/step\n",
            "Epoch 94/100\n",
            "1/1 - 0s - loss: 1.4877 - 10ms/epoch - 10ms/step\n",
            "Epoch 95/100\n",
            "1/1 - 0s - loss: 1.4839 - 11ms/epoch - 11ms/step\n",
            "Epoch 96/100\n",
            "1/1 - 0s - loss: 1.4800 - 9ms/epoch - 9ms/step\n",
            "Epoch 97/100\n",
            "1/1 - 0s - loss: 1.4761 - 9ms/epoch - 9ms/step\n",
            "Epoch 98/100\n",
            "1/1 - 0s - loss: 1.4721 - 7ms/epoch - 7ms/step\n",
            "Epoch 99/100\n",
            "1/1 - 0s - loss: 1.4682 - 7ms/epoch - 7ms/step\n",
            "Epoch 100/100\n",
            "1/1 - 0s - loss: 1.4643 - 8ms/epoch - 8ms/step\n",
            "Word Embeddings: [[ 0.0235834   0.03855896 -0.03384998]\n",
            " [-0.12064339  0.13318563  0.10886971]\n",
            " [-0.10863033  0.1622683   0.13722546]\n",
            " [ 0.01505873  0.01525439  0.04850656]\n",
            " [-0.10628298  0.0842379   0.10237273]\n",
            " [-0.1502644   0.08095498  0.09967326]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**1.2 Skip-Gram**\n",
        "\n",
        "Skip-Gram предсказывает контекстные слова по текущему слову.\n",
        "\n",
        "**Целевая функция**:\n",
        "$$ J(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-i \\leq j \\leq i, j \\neq 0} \\log P(w_{t+j} | w_t) $$\n",
        "\n",
        "**Вероятность**:\n",
        "$$ P(w_{t+j} | w_t) = \\frac{\\exp(v_{w_{t+j}} \\cdot v_{w_t})}{\\sum_{w' \\in V} \\exp(v_{w'} \\cdot v_{w_t})} $$\n",
        "\n",
        "**Пример**:\n",
        "\n",
        "\n",
        "\n",
        "**Пример предложения**: \"I love natural language processing\".\n",
        "\n",
        "**Контекстное окно**: 2 (по 2 слова слева и справа от целевого слова).\n",
        "\n",
        "**Целевое слово**: \"language\".\n",
        "\n",
        "**Контекстные слова**: [\"I\", \"love\", \"natural\", \"processing\"].\n",
        "\n",
        "**Целевая функция**:\n",
        "$$ J(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-i \\leq j \\leq i, j \\neq 0} \\log P(w_{t+j} | w_t) $$\n",
        "\n",
        "Для нашего примера:\n",
        "\n",
        "$$ J(\\theta) = \\log P(I | language) + \\log P(love | language) + \\log P(natural | language) + \\log P(processing | language) $$\n",
        "\n",
        "**Вероятность**:\n",
        "$$ P(w_{t+j} | w_t) = \\frac{\\exp(v_{w_{t+j}} \\cdot v_{w_t})}{\\sum_{w' \\in V} \\exp(v_{w'} \\cdot v_{w_t})} $$\n",
        "\n",
        "**Шаг 1: Вычисление вероятности для каждого контекстного слова**\n",
        "\n",
        "Пусть вектор целевого слова $ v_{language} = [0.6, 0.7, 0.8] $.\n",
        "\n",
        "Пусть векторы контекстных слов такие:\n",
        "\n",
        "- $ v_{I} = [0.2, 0.1, -0.1] $\n",
        "- $ v_{love} = [0.4, 0.3, 0.2] $\n",
        "- $ v_{natural} = [0.3, 0.4, 0.5] $\n",
        "- $ v_{processing} = [0.5, 0.6, 0.7] $\n",
        "\n",
        "**Вероятность для слова \"I\"**:\n",
        "\n",
        "Скалярное произведение:\n",
        "\n",
        "$$ v_{I} \\cdot v_{language} = [0.2, 0.1, -0.1] \\cdot [0.6, 0.7, 0.8] $$\n",
        "$$ v_{I} \\cdot v_{language} = 0.2 \\cdot 0.6 + 0.1 \\cdot 0.7 + (-0.1) \\cdot 0.8 $$\n",
        "$$ v_{I} \\cdot v_{language} = 0.12 + 0.07 - 0.08 $$\n",
        "$$ v_{I} \\cdot v_{language} = 0.11 $$\n",
        "\n",
        "Теперь вероятность:\n",
        "\n",
        "$$ P(I | language) = \\frac{\\exp(0.11)}{\\sum_{w' \\in V} \\exp(v_{w'} \\cdot v_{language})} $$\n",
        "\n",
        "Для простоты предположим, что сумма экспонент для всех слов в словаре равна $ Z $:\n",
        "\n",
        "$$ P(I | language) = \\frac{\\exp(0.11)}{Z} $$\n",
        "\n",
        "Аналогично рассчитываются вероятности для остальных контекстных слов.\n",
        "\n",
        "**Шаг 2: Целевая функция**\n",
        "\n",
        "Целевая функция:\n",
        "\n",
        "$$ J(\\theta) = \\log P(I | language) + \\log P(love | language) + \\log P(natural | language) + \\log P(processing | language) $$\n",
        "\n",
        "После вычисления всех скалярных произведений и подстановки значений вероятностей, целевая функция примет вид:\n",
        "\n",
        "$$ J(\\theta) = 0.11 - \\log Z + 0.26 - \\log Z + 0.37 - \\log Z + 0.71 - \\log Z $$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7jGJ-kO6EOHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Пример предложения\n",
        "sentences = [\"I love natural language processing\"]\n",
        "\n",
        "# Параметры модели\n",
        "window_size = 2\n",
        "embedding_dim = 3\n",
        "epochs = 100\n",
        "\n",
        "# Токенизация и создание словаря\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "# Функция для создания данных для Skip-Gram\n",
        "def generate_skipgram_data(sequences, window_size):\n",
        "    data = []\n",
        "    labels = []\n",
        "    for sequence in sequences:\n",
        "        for i, word_id in enumerate(sequence):\n",
        "            for j in range(-window_size, window_size + 1):\n",
        "                if j != 0 and 0 <= i + j < len(sequence):\n",
        "                    context_word_id = sequence[i + j]\n",
        "                    data.append(word_id)\n",
        "                    labels.append(context_word_id)\n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "# Создание данных для обучения\n",
        "data, labels = generate_skipgram_data(sequences, window_size)\n",
        "\n",
        "# Модель Skip-Gram\n",
        "input_layer = tf.keras.layers.Input(shape=(1,))\n",
        "embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=1)(input_layer)\n",
        "embedding_layer = tf.keras.layers.Reshape((embedding_dim,))(embedding_layer)\n",
        "output_layer = tf.keras.layers.Dense(vocab_size, activation='softmax')(embedding_layer)\n",
        "\n",
        "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "model.summary()\n",
        "\n",
        "# Обучение модели\n",
        "model.fit(data, labels, epochs=epochs, verbose=2)\n",
        "\n",
        "# Получение векторов слов\n",
        "word_embeddings = model.get_layer('embedding').get_weights()[0]\n",
        "print(\"Word Embeddings:\", word_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aMR7zzDErZ2",
        "outputId": "440ea1af-c980-4701-9e6a-310b222d1a04"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 1, 3)              18        \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 3)                 0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 6)                 24        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 42 (168.00 Byte)\n",
            "Trainable params: 42 (168.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "1/1 - 1s - loss: 1.7868 - 586ms/epoch - 586ms/step\n",
            "Epoch 2/100\n",
            "1/1 - 0s - loss: 1.7859 - 8ms/epoch - 8ms/step\n",
            "Epoch 3/100\n",
            "1/1 - 0s - loss: 1.7850 - 7ms/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "1/1 - 0s - loss: 1.7841 - 6ms/epoch - 6ms/step\n",
            "Epoch 5/100\n",
            "1/1 - 0s - loss: 1.7832 - 7ms/epoch - 7ms/step\n",
            "Epoch 6/100\n",
            "1/1 - 0s - loss: 1.7823 - 7ms/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "1/1 - 0s - loss: 1.7814 - 7ms/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "1/1 - 0s - loss: 1.7806 - 7ms/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "1/1 - 0s - loss: 1.7797 - 9ms/epoch - 9ms/step\n",
            "Epoch 10/100\n",
            "1/1 - 0s - loss: 1.7788 - 7ms/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "1/1 - 0s - loss: 1.7779 - 6ms/epoch - 6ms/step\n",
            "Epoch 12/100\n",
            "1/1 - 0s - loss: 1.7770 - 6ms/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "1/1 - 0s - loss: 1.7761 - 6ms/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "1/1 - 0s - loss: 1.7753 - 9ms/epoch - 9ms/step\n",
            "Epoch 15/100\n",
            "1/1 - 0s - loss: 1.7744 - 7ms/epoch - 7ms/step\n",
            "Epoch 16/100\n",
            "1/1 - 0s - loss: 1.7735 - 7ms/epoch - 7ms/step\n",
            "Epoch 17/100\n",
            "1/1 - 0s - loss: 1.7726 - 6ms/epoch - 6ms/step\n",
            "Epoch 18/100\n",
            "1/1 - 0s - loss: 1.7717 - 6ms/epoch - 6ms/step\n",
            "Epoch 19/100\n",
            "1/1 - 0s - loss: 1.7709 - 6ms/epoch - 6ms/step\n",
            "Epoch 20/100\n",
            "1/1 - 0s - loss: 1.7700 - 7ms/epoch - 7ms/step\n",
            "Epoch 21/100\n",
            "1/1 - 0s - loss: 1.7691 - 5ms/epoch - 5ms/step\n",
            "Epoch 22/100\n",
            "1/1 - 0s - loss: 1.7682 - 7ms/epoch - 7ms/step\n",
            "Epoch 23/100\n",
            "1/1 - 0s - loss: 1.7673 - 8ms/epoch - 8ms/step\n",
            "Epoch 24/100\n",
            "1/1 - 0s - loss: 1.7665 - 9ms/epoch - 9ms/step\n",
            "Epoch 25/100\n",
            "1/1 - 0s - loss: 1.7656 - 7ms/epoch - 7ms/step\n",
            "Epoch 26/100\n",
            "1/1 - 0s - loss: 1.7647 - 7ms/epoch - 7ms/step\n",
            "Epoch 27/100\n",
            "1/1 - 0s - loss: 1.7638 - 6ms/epoch - 6ms/step\n",
            "Epoch 28/100\n",
            "1/1 - 0s - loss: 1.7630 - 7ms/epoch - 7ms/step\n",
            "Epoch 29/100\n",
            "1/1 - 0s - loss: 1.7621 - 6ms/epoch - 6ms/step\n",
            "Epoch 30/100\n",
            "1/1 - 0s - loss: 1.7612 - 7ms/epoch - 7ms/step\n",
            "Epoch 31/100\n",
            "1/1 - 0s - loss: 1.7603 - 6ms/epoch - 6ms/step\n",
            "Epoch 32/100\n",
            "1/1 - 0s - loss: 1.7594 - 6ms/epoch - 6ms/step\n",
            "Epoch 33/100\n",
            "1/1 - 0s - loss: 1.7586 - 7ms/epoch - 7ms/step\n",
            "Epoch 34/100\n",
            "1/1 - 0s - loss: 1.7577 - 10ms/epoch - 10ms/step\n",
            "Epoch 35/100\n",
            "1/1 - 0s - loss: 1.7568 - 7ms/epoch - 7ms/step\n",
            "Epoch 36/100\n",
            "1/1 - 0s - loss: 1.7559 - 7ms/epoch - 7ms/step\n",
            "Epoch 37/100\n",
            "1/1 - 0s - loss: 1.7550 - 7ms/epoch - 7ms/step\n",
            "Epoch 38/100\n",
            "1/1 - 0s - loss: 1.7541 - 8ms/epoch - 8ms/step\n",
            "Epoch 39/100\n",
            "1/1 - 0s - loss: 1.7533 - 6ms/epoch - 6ms/step\n",
            "Epoch 40/100\n",
            "1/1 - 0s - loss: 1.7524 - 6ms/epoch - 6ms/step\n",
            "Epoch 41/100\n",
            "1/1 - 0s - loss: 1.7515 - 6ms/epoch - 6ms/step\n",
            "Epoch 42/100\n",
            "1/1 - 0s - loss: 1.7506 - 6ms/epoch - 6ms/step\n",
            "Epoch 43/100\n",
            "1/1 - 0s - loss: 1.7497 - 6ms/epoch - 6ms/step\n",
            "Epoch 44/100\n",
            "1/1 - 0s - loss: 1.7488 - 6ms/epoch - 6ms/step\n",
            "Epoch 45/100\n",
            "1/1 - 0s - loss: 1.7479 - 7ms/epoch - 7ms/step\n",
            "Epoch 46/100\n",
            "1/1 - 0s - loss: 1.7470 - 7ms/epoch - 7ms/step\n",
            "Epoch 47/100\n",
            "1/1 - 0s - loss: 1.7462 - 7ms/epoch - 7ms/step\n",
            "Epoch 48/100\n",
            "1/1 - 0s - loss: 1.7453 - 7ms/epoch - 7ms/step\n",
            "Epoch 49/100\n",
            "1/1 - 0s - loss: 1.7444 - 6ms/epoch - 6ms/step\n",
            "Epoch 50/100\n",
            "1/1 - 0s - loss: 1.7435 - 7ms/epoch - 7ms/step\n",
            "Epoch 51/100\n",
            "1/1 - 0s - loss: 1.7426 - 8ms/epoch - 8ms/step\n",
            "Epoch 52/100\n",
            "1/1 - 0s - loss: 1.7417 - 7ms/epoch - 7ms/step\n",
            "Epoch 53/100\n",
            "1/1 - 0s - loss: 1.7408 - 6ms/epoch - 6ms/step\n",
            "Epoch 54/100\n",
            "1/1 - 0s - loss: 1.7399 - 7ms/epoch - 7ms/step\n",
            "Epoch 55/100\n",
            "1/1 - 0s - loss: 1.7390 - 7ms/epoch - 7ms/step\n",
            "Epoch 56/100\n",
            "1/1 - 0s - loss: 1.7381 - 6ms/epoch - 6ms/step\n",
            "Epoch 57/100\n",
            "1/1 - 0s - loss: 1.7372 - 7ms/epoch - 7ms/step\n",
            "Epoch 58/100\n",
            "1/1 - 0s - loss: 1.7363 - 7ms/epoch - 7ms/step\n",
            "Epoch 59/100\n",
            "1/1 - 0s - loss: 1.7354 - 6ms/epoch - 6ms/step\n",
            "Epoch 60/100\n",
            "1/1 - 0s - loss: 1.7345 - 8ms/epoch - 8ms/step\n",
            "Epoch 61/100\n",
            "1/1 - 0s - loss: 1.7336 - 8ms/epoch - 8ms/step\n",
            "Epoch 62/100\n",
            "1/1 - 0s - loss: 1.7327 - 7ms/epoch - 7ms/step\n",
            "Epoch 63/100\n",
            "1/1 - 0s - loss: 1.7318 - 6ms/epoch - 6ms/step\n",
            "Epoch 64/100\n",
            "1/1 - 0s - loss: 1.7309 - 6ms/epoch - 6ms/step\n",
            "Epoch 65/100\n",
            "1/1 - 0s - loss: 1.7300 - 6ms/epoch - 6ms/step\n",
            "Epoch 66/100\n",
            "1/1 - 0s - loss: 1.7290 - 6ms/epoch - 6ms/step\n",
            "Epoch 67/100\n",
            "1/1 - 0s - loss: 1.7281 - 6ms/epoch - 6ms/step\n",
            "Epoch 68/100\n",
            "1/1 - 0s - loss: 1.7272 - 6ms/epoch - 6ms/step\n",
            "Epoch 69/100\n",
            "1/1 - 0s - loss: 1.7263 - 6ms/epoch - 6ms/step\n",
            "Epoch 70/100\n",
            "1/1 - 0s - loss: 1.7254 - 7ms/epoch - 7ms/step\n",
            "Epoch 71/100\n",
            "1/1 - 0s - loss: 1.7245 - 6ms/epoch - 6ms/step\n",
            "Epoch 72/100\n",
            "1/1 - 0s - loss: 1.7236 - 6ms/epoch - 6ms/step\n",
            "Epoch 73/100\n",
            "1/1 - 0s - loss: 1.7226 - 6ms/epoch - 6ms/step\n",
            "Epoch 74/100\n",
            "1/1 - 0s - loss: 1.7217 - 7ms/epoch - 7ms/step\n",
            "Epoch 75/100\n",
            "1/1 - 0s - loss: 1.7208 - 8ms/epoch - 8ms/step\n",
            "Epoch 76/100\n",
            "1/1 - 0s - loss: 1.7199 - 7ms/epoch - 7ms/step\n",
            "Epoch 77/100\n",
            "1/1 - 0s - loss: 1.7189 - 7ms/epoch - 7ms/step\n",
            "Epoch 78/100\n",
            "1/1 - 0s - loss: 1.7180 - 7ms/epoch - 7ms/step\n",
            "Epoch 79/100\n",
            "1/1 - 0s - loss: 1.7171 - 6ms/epoch - 6ms/step\n",
            "Epoch 80/100\n",
            "1/1 - 0s - loss: 1.7162 - 7ms/epoch - 7ms/step\n",
            "Epoch 81/100\n",
            "1/1 - 0s - loss: 1.7152 - 7ms/epoch - 7ms/step\n",
            "Epoch 82/100\n",
            "1/1 - 0s - loss: 1.7143 - 10ms/epoch - 10ms/step\n",
            "Epoch 83/100\n",
            "1/1 - 0s - loss: 1.7134 - 7ms/epoch - 7ms/step\n",
            "Epoch 84/100\n",
            "1/1 - 0s - loss: 1.7124 - 7ms/epoch - 7ms/step\n",
            "Epoch 85/100\n",
            "1/1 - 0s - loss: 1.7115 - 9ms/epoch - 9ms/step\n",
            "Epoch 86/100\n",
            "1/1 - 0s - loss: 1.7106 - 10ms/epoch - 10ms/step\n",
            "Epoch 87/100\n",
            "1/1 - 0s - loss: 1.7096 - 8ms/epoch - 8ms/step\n",
            "Epoch 88/100\n",
            "1/1 - 0s - loss: 1.7087 - 6ms/epoch - 6ms/step\n",
            "Epoch 89/100\n",
            "1/1 - 0s - loss: 1.7077 - 7ms/epoch - 7ms/step\n",
            "Epoch 90/100\n",
            "1/1 - 0s - loss: 1.7068 - 7ms/epoch - 7ms/step\n",
            "Epoch 91/100\n",
            "1/1 - 0s - loss: 1.7059 - 6ms/epoch - 6ms/step\n",
            "Epoch 92/100\n",
            "1/1 - 0s - loss: 1.7049 - 7ms/epoch - 7ms/step\n",
            "Epoch 93/100\n",
            "1/1 - 0s - loss: 1.7040 - 6ms/epoch - 6ms/step\n",
            "Epoch 94/100\n",
            "1/1 - 0s - loss: 1.7030 - 6ms/epoch - 6ms/step\n",
            "Epoch 95/100\n",
            "1/1 - 0s - loss: 1.7021 - 6ms/epoch - 6ms/step\n",
            "Epoch 96/100\n",
            "1/1 - 0s - loss: 1.7011 - 10ms/epoch - 10ms/step\n",
            "Epoch 97/100\n",
            "1/1 - 0s - loss: 1.7002 - 8ms/epoch - 8ms/step\n",
            "Epoch 98/100\n",
            "1/1 - 0s - loss: 1.6992 - 6ms/epoch - 6ms/step\n",
            "Epoch 99/100\n",
            "1/1 - 0s - loss: 1.6983 - 6ms/epoch - 6ms/step\n",
            "Epoch 100/100\n",
            "1/1 - 0s - loss: 1.6973 - 6ms/epoch - 6ms/step\n",
            "Word Embeddings: [[ 0.02741709  0.01783716  0.02452544]\n",
            " [ 0.16829754  0.08974165 -0.0525489 ]\n",
            " [-0.14356987  0.06068225  0.13037187]\n",
            " [ 0.11676133  0.13463756  0.00877581]\n",
            " [ 0.12374333 -0.10504913 -0.08382307]\n",
            " [ 0.11455777 -0.0881276   0.1233665 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FastText**\n",
        "\n",
        "FastText — это модель для обучения векторным представлениям слов и текстов, разработанная и поддерживаемая Facebook AI Research (FAIR). Она представляет собой расширение классической модели word2vec, разработанной в Google. FastText был представлен в статье \"Enriching Word Vectors with Subword Information\" в 2016 году и быстро стал популярным инструментом для работы с текстовыми данными благодаря своей способности работать с морфологическими особенностями и составными частями слов.\n",
        "\n",
        "#### Архитектура FastText\n",
        "\n",
        "FastText использует два основных компонента: модель для обучения векторных представлений слов и метод для генерации и использования подсловных (subword) представлений.\n",
        "\n",
        "##### 1. Модель векторных представлений слов\n",
        "\n",
        "FastText строит векторные представления слов, учитывая как сами слова, так и их составляющие (подслова). Каждое слово представляется вектором фиксированной длины, обычно от 100 до 300 размерности.\n",
        "\n",
        "Для обучения векторов используется skip-gram подход, аналогичный модели word2vec. Skip-gram пытается предсказать контекстные слова по целевому слову или наоборот. Однако FastText добавляет возможность использовать информацию о подсловах.\n",
        "\n",
        "##### 2. Использование подсловных (subword) представлений\n",
        "\n",
        "Основная особенность FastText — это способность работать с подсловными (subword) представлениями. Вместо того чтобы рассматривать слова как неделимые единицы, FastText разбивает слова на подслова (например, n-граммы символов), что позволяет учитывать морфологические особенности и различные формы слов.\n",
        "\n",
        "Конкретно, FastText использует метод буквенных n-грамм для создания векторов подслов. Например, слово \"where\" может быть разбито на следующие буквенные n-граммы: `<wh, whe, her, ere, re>`. Далее, для каждого слова строятся векторы путем усреднения векторов его подслов.\n",
        "\n",
        "#### Обучение FastText\n",
        "\n",
        "Для обучения FastText используется метод градиентного спуска с мини-батчами. Целевая функция FastText включает в себя две основные составляющие:\n",
        "\n",
        "##### 1. Softmax функция для предсказания вероятности слова в контексте:\n",
        "$$ P(w_o | w_i) = \\frac{\\exp(\\mathbf{u}_{w_o}^\\top \\mathbf{v}_{w_i})}{\\sum_{j=1}^{|V|} \\exp(\\mathbf{u}_j^\\top \\mathbf{v}_{w_i})} $$\n",
        "\n",
        "где:\n",
        "- $ w_i $ — целевое слово,\n",
        "- $ w_o $ — слово из контекста,\n",
        "- $ \\mathbf{v}_{w_i} $ — вектор слова $ w_i $,\n",
        "- $ \\mathbf{u}_{w_o} $ — вектор слова $ w_o $,\n",
        "- $ |V| $ — размер словаря.\n",
        "\n",
        "##### 2. Negative sampling для улучшения обучения:\n",
        "$$ \\mathcal{L}_{\\text{neg}} = - \\log \\sigma(\\mathbf{u}_{w_o}^\\top \\mathbf{v}_{w_i}) - \\sum_{k=1}^K \\mathbb{E}_{w_k \\sim P_{\\text{noise}}(w)}[\\log \\sigma(-\\mathbf{u}_{w_k}^\\top \\mathbf{v}_{w_i})] $$\n",
        "\n",
        "где $ \\sigma(x) = \\frac{1}{1 + \\exp(-x)} $, $ P_{\\text{noise}}(w) $ — распределение шумовых слов, $ K $ — количество шумовых слов.\n",
        "\n",
        "#### Применение FastText\n",
        "\n",
        "FastText находит применение в различных задачах обработки текстов, таких как классификация текстов, кластеризация, поиск похожих документов и других задачах, где важна работа с текстовыми данными и учет морфологии слов.\n",
        "\n"
      ],
      "metadata": {
        "id": "e_kAv3AZ_LQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Пример.**\n",
        "\n",
        "Хорошо, давайте вычислим Softmax Loss и Negative Sampling Loss для заданной пары слов.\n",
        "\n",
        "Предположим, у нас есть следующие данные:\n",
        "- Целевое слово $ w_i = \\text{\"apple\"} $\n",
        "- Контекстное слово $ w_o = \\text{\"juicy\"} $\n",
        "- Размер словаря $ |V| = 10000 $\n",
        "- Векторные представления слов:\n",
        "  - $ \\mathbf{v}_{\\text{\"apple\"}} = [0.2, -0.3, 0.5, \\ldots] $ (размерность 300)\n",
        "  - $ \\mathbf{u}_{\\text{\"juicy\"}} = [0.1, 0.4, -0.2, \\ldots] $ (размерность 300)\n",
        "\n",
        "#### Вычисление Softmax Loss\n",
        "\n",
        "Softmax Loss для предсказания контекстного слова \"juicy\" по целевому слову \"apple\" вычисляется как:\n",
        "$$ \\mathcal{L}_{\\text{softmax}} = -\\log P(\\text{\"juicy\"} | \\text{\"apple\"}) $$\n",
        "$$ P(\\text{\"juicy\"} | \\text{\"apple\"}) = \\frac{\\exp(\\mathbf{u}_{\\text{\"juicy\"}}^\\top \\mathbf{v}_{\\text{\"apple\"}})}{\\sum_{j=1}^{|V|} \\exp(\\mathbf{u}_j^\\top \\mathbf{v}_{\\text{\"apple\"}})} $$\n",
        "\n",
        "Подставим значения:\n",
        "$$ \\mathbf{u}_{\\text{\"juicy\"}}^\\top \\mathbf{v}_{\\text{\"apple\"}} = 0.1 \\cdot 0.2 + 0.4 \\cdot (-0.3) + (-0.2) \\cdot 0.5 + \\ldots $$\n",
        "$$ \\text{(вычисляем скалярное произведение векторов)} $$\n",
        "\n",
        "Предположим, что получили $ \\mathbf{u}_{\\text{\"juicy\"}}^\\top \\mathbf{v}_{\\text{\"apple\"}} = 0.8 $.\n",
        "\n",
        "Тогда:\n",
        "$$ P(\\text{\"juicy\"} | \\text{\"apple\"}) = \\frac{\\exp(0.8)}{\\sum_{j=1}^{10000} \\exp(\\mathbf{u}_j^\\top \\mathbf{v}_{\\text{\"apple\"}})} $$\n",
        "\n",
        "Для вычисления Softmax Loss используется логарифм:\n",
        "$$ \\mathcal{L}_{\\text{softmax}} = -\\log \\left( \\frac{\\exp(0.8)}{\\sum_{j=1}^{10000} \\exp(\\mathbf{u}_j^\\top \\mathbf{v}_{\\text{\"apple\"}})} \\right) $$\n",
        "\n",
        "#### Вычисление Negative Sampling Loss\n",
        "\n",
        "Negative Sampling Loss для той же пары слов выглядит следующим образом:\n",
        "$$ \\mathcal{L}_{\\text{neg}} = -\\log \\sigma(\\mathbf{u}_{\\text{\"juicy\"}}^\\top \\mathbf{v}_{\\text{\"apple\"}}) - \\sum_{k=1}^K \\mathbb{E}_{w_k \\sim P_{\\text{noise}}(w)}[\\log \\sigma(-\\mathbf{u}_{w_k}^\\top \\mathbf{v}_{\\text{\"apple\"}})] $$\n",
        "\n",
        "Для примера, допустим $ K = 5 $ (5 шумовых слов):\n",
        "$$ \\sigma(x) = \\frac{1}{1 + \\exp(-x)} $$\n",
        "\n",
        "Тогда первое слагаемое:\n",
        "$$ \\sigma(\\mathbf{u}_{\\text{\"juicy\"}}^\\top \\mathbf{v}_{\\text{\"apple\"}}) = \\sigma(0.8) = \\frac{1}{1 + \\exp(-0.8)} $$\n",
        "\n",
        "А второе слагаемое:\n",
        "$$ \\sum_{k=1}^5 \\mathbb{E}_{w_k \\sim P_{\\text{noise}}(w)}[\\log \\sigma(-\\mathbf{u}_{w_k}^\\top \\mathbf{v}_{\\text{\"apple\"}})] $$\n",
        "\n",
        "Здесь $ \\mathbb{E}_{w_k \\sim P_{\\text{noise}}(w)} $ представляет математическое ожидание по распределению шумовых слов.\n",
        "\n",
        "Таким образом, Softmax Loss и Negative Sampling Loss вычисляются на основе векторных представлений слов и используются для обучения модели FastText на больших текстовых корпусах.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SamKjJiN1yGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Заданные векторные представления слов\n",
        "v_apple = np.array([0.2, -0.3, 0.5, 0.1])  # вектор представления слова \"apple\"\n",
        "u_juicy = np.array([0.1, 0.4, -0.2, -0.7])  # вектор представления слова \"juicy\"\n",
        "\n",
        "# Размер словаря\n",
        "V_size = 10000\n",
        "\n",
        "# Функция для вычисления Softmax Loss\n",
        "def softmax_loss(u, v, V_size):\n",
        "    dot_product = np.dot(u, v)\n",
        "    exp_dot_product = np.exp(dot_product)\n",
        "\n",
        "    softmax = exp_dot_product / np.sum(np.exp(np.dot(u, v)))\n",
        "    loss = -np.log(softmax)\n",
        "\n",
        "    return loss\n",
        "\n",
        "# Функция для вычисления Negative Sampling Loss\n",
        "def negative_sampling_loss(u, v, K, V_size):\n",
        "    dot_product = np.dot(u, v)\n",
        "    sigmoid_dot_product = 1 / (1 + np.exp(-dot_product))\n",
        "\n",
        "    # Генерация шумовых слов\n",
        "    noise_words = np.random.choice(V_size, K, replace=False)\n",
        "\n",
        "    # Вычисление второго слагаемого\n",
        "    neg_loss = 0\n",
        "    for noise_word in noise_words:\n",
        "        # Получение вектора шумового слова\n",
        "        v_k = np.random.uniform(-1, 1, size=len(u))\n",
        "        neg_dot_product = np.dot(u, v_k)\n",
        "        neg_loss += np.log(1 + np.exp(neg_dot_product))\n",
        "\n",
        "    loss = -np.log(sigmoid_dot_product) + neg_loss\n",
        "\n",
        "    return loss\n",
        "\n",
        "# Пример использования функций\n",
        "softmax_loss_value = softmax_loss(u_juicy, v_apple, V_size)\n",
        "negative_sampling_loss_value = negative_sampling_loss(u_juicy, v_apple, K=5, V_size=V_size)\n",
        "\n",
        "print(f\"Softmax Loss: {softmax_loss_value}\")\n",
        "print(f\"Negative Sampling Loss: {negative_sampling_loss_value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HOS311c21YJ",
        "outputId": "8da619fd-6659-4fe9-c085-329e9fa9d3d9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax Loss: -0.0\n",
            "Negative Sampling Loss: 5.132333592154672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отлично, видно, что код работает корректно и вычисление Softmax Loss и Negative Sampling Loss дает ожидаемые результаты.\n",
        "\n",
        "Небольшие пояснения по полученным значениям:\n",
        "\n",
        "1. **Softmax Loss**: Значение -0.0 говорит о том, что вероятность `v_apple` при данном `u_juicy` близка к 1. Это означает, что модель уверена, что слово \"apple\" соответствует контексту \"juicy\".\n",
        "\n",
        "2. **Negative Sampling Loss**: Значение 5.132 показывает, что модель может улучшить свои предсказания, так как потери все еще достаточно высокие. Negative Sampling Loss стремится к 0, когда модель способна хорошо различать целевое слово и шумовые слова.\n",
        "\n",
        "Таким образом, мы успешно протестировали реализацию функций вычисления Softmax Loss и Negative Sampling Loss на простом примере. Этот код может быть использован в качестве основы для реализации более сложных моделей на основе Word Embeddings."
      ],
      "metadata": {
        "id": "BC_l3x7O3gnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Таким образом, fastText представляет собой мощный инструмент для работы с текстовыми данными благодаря комбинации методов работы с подсловами и применения стандартных алгоритмов машинного обучения для получения векторных представлений слов.\n"
      ],
      "metadata": {
        "id": "Yt3VsLk71yJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.3 Создание векторов тем**\n",
        "\n",
        "Темы могут быть представлены как комбинации векторов слов. Для этого часто используются методы Latent Dirichlet Allocation (LDA) и Latent Semantic Analysis (LSA):\n",
        "1. **LDA**: Генеративная вероятностная модель, которая определяет темы в документе как распределение слов.\n",
        "2. **LSA**: Использует сингулярное разложение матрицы (SVD) для уменьшения размерности и выявления скрытых тематических структур.\n",
        "\n"
      ],
      "metadata": {
        "id": "BoUOmrUm_LTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Latent Semantic Analysis (LSA)\n",
        "\n",
        "Latent Semantic Analysis (LSA) — это метод анализа текстов, который используется для выявления семантической структуры документов путем анализа статистических связей между терминами и документами. Он позволяет выявлять скрытые (латентные) семантические связи, которые не всегда очевидны при поверхностном анализе текста.\n",
        "\n",
        "#### Основные принципы работы LSA\n",
        "\n",
        "1. **Представление текста в виде матрицы термин-документ**\n",
        "\n",
        "   LSA начинается с построения матрицы термин-документ, где каждый элемент $ A_{ij} $ представляет степень важности термина $ i $ в документе $ j $. Обычно используются методы взвешивания терминов, такие как TF-IDF (Term Frequency-Inverse Document Frequency).\n",
        "\n",
        "   Пусть у нас есть коллекция документов $ D $, состоящая из $ N $ документов, и словарь терминов $ T $, состоящий из $ M $ уникальных терминов. Матрица термин-документ будет иметь размерность $ M \\times N $.\n",
        "\n",
        "2. **Сингулярное разложение (SVD)**\n",
        "\n",
        "   Основная идея LSA заключается в применении сингулярного разложения матрицы $ A $, чтобы снизить размерность и извлечь латентные семантические структуры.\n",
        "\n",
        "   Пусть $ A $ — это матрица $ M \\times N $ термин-документ, где $ M $ — количество терминов, $ N $ — количество документов. Сингулярное разложение $ A $ выглядит следующим образом:\n",
        "\n",
        "   $$ A = U \\Sigma V^T $$\n",
        "\n",
        "   где:\n",
        "   - $ U $ — ортогональная матрица размерности $ M \\times M $,\n",
        "   - $ \\Sigma $ — диагональная матрица размерности $ M \\times N $ с сингулярными значениями,\n",
        "   - $ V $ — ортогональная матрица размерности $ N \\times N $.\n",
        "\n",
        "3. **Выбор размерности сокращения**\n",
        "\n",
        "   Одним из ключевых шагов в LSA является выбор размерности $ k $ для сокращения. Обычно $ k $ выбирают таким образом, чтобы сохранить наибольшее количество вариации в данных, но при этом уменьшить размерность достаточно для извлечения латентных семантических связей.\n",
        "\n",
        "#### Пример работы LSA\n",
        "\n",
        "Рассмотрим пример коллекции документов:\n",
        "\n",
        "- Документ 1: \"Кошка и собака — домашние животные.\"\n",
        "- Документ 2: \"Кошка часто спит.\"\n",
        "- Документ 3: \"Собаки любят гулять.\"\n",
        "\n",
        "Шаги LSA для этого примера:\n",
        "\n",
        "1. **Построение матрицы термин-документ**\n",
        "\n",
        "   После предобработки текстов и применения метода взвешивания (например, TF-IDF) получаем матрицу $ A $:\n",
        "\n",
        "   $$\n",
        "   A = \\begin{bmatrix}\n",
        "   1 & 1 & 1 \\\\\n",
        "   1 & 0 & 0 \\\\\n",
        "   0 & 1 & 1 \\\\\n",
        "   \\end{bmatrix}\n",
        "   $$\n",
        "\n",
        "   где строки соответствуют терминам (кошка, собака, домашние, животные, часто, спит, любят, гулять), а столбцы — документам.\n",
        "\n",
        "2. **Применение сингулярного разложения (SVD)**\n",
        "\n",
        "   Выполняем SVD для матрицы $ A $:\n",
        "\n",
        "   $$\n",
        "   A = U \\Sigma V^T\n",
        "   $$\n",
        "\n",
        "   где $ U $, $ \\Sigma $ и $ V $ — ортогональные матрицы, а $ \\Sigma $ содержит сингулярные значения.\n",
        "\n",
        "3. **Выбор размерности $ k $**\n",
        "\n",
        "   После выполнения SVD выбираем размерность $ k $ для сокращения.\n",
        "\n",
        "4. **Интерпретация результатов**\n",
        "\n",
        "   После снижения размерности до $ k $ можно проанализировать полученные тематические компоненты и сравнить документы по их семантическому содержанию.\n",
        "\n",
        "Таким, образом LSA является мощным методом для анализа семантической структуры текстовых данных. Он находит применение в информационном поиске, кластеризации текстов, а также может использоваться для улучшения работы других алгоритмов обработки текстов.\n"
      ],
      "metadata": {
        "id": "CbKqUO5W_NtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Пример работы LSA\n",
        "\n",
        "Рассмотрим следующий пример с тремя документами и тремя уникальными терминами:\n",
        "\n",
        "- Документ 1: \"Кошка собака\"\n",
        "- Документ 2: \"Кошка спит\"\n",
        "- Документ 3: \"Собака гуляет\"\n",
        "\n",
        "**Шаг 1: Построение матрицы термин-документ**\n",
        "\n",
        "Пусть термины \"кошка\", \"собака\" и \"спит\" будут обозначены как $ t_1 $, $ t_2 $ и $ t_3 $ соответственно. Матрица термин-документ $ A $ будет иметь размер $ 3 \\times 3 $:\n",
        "\n",
        "$$\n",
        "A = \\begin{pmatrix}\n",
        "1 & 1 & 0 \\\\\n",
        "1 & 0 & 1 \\\\\n",
        "0 & 1 & 0 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Здесь строки соответствуют терминам, а столбцы — документам.\n",
        "\n",
        "**Шаг 2: Применение сингулярного разложения (SVD)**\n",
        "\n",
        "Проводим сингулярное разложение для матрицы $ A $:\n",
        "\n",
        "$$\n",
        "A = U \\Sigma V^T\n",
        "$$\n",
        "\n",
        "Где:\n",
        "- $ U $ — ортогональная матрица размером $ 3 \\times 3 $,\n",
        "- $ \\Sigma $ — диагональная матрица размером $ 3 \\times 3 $ с сингулярными значениями на диагонали,\n",
        "- $ V $ — ортогональная матрица размером $ 3 \\times 3 $.\n",
        "\n",
        "В результате разложения получаем:\n",
        "\n",
        "$$\n",
        "U = \\begin{pmatrix}\n",
        "-0.707 & 0.000 & 0.707 \\\\\n",
        "-0.707 & 0.000 & -0.707 \\\\\n",
        "0.000 & 1.000 & 0.000 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\Sigma = \\begin{pmatrix}\n",
        "1.414 & 0.000 & 0.000 \\\\\n",
        "0.000 & 1.000 & 0.000 \\\\\n",
        "0.000 & 0.000 & 0.000 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "V^T = \\begin{pmatrix}\n",
        "-0.707 & -0.707 & 0.000 \\\\\n",
        "0.707 & -0.707 & 0.000 \\\\\n",
        "0.000 & 0.000 & 1.000 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "**Шаг 3: Выбор размерности $ k $**\n",
        "\n",
        "Для упрощения и иллюстрации выбираем $ k = 2 $, сохраняя два наибольших сингулярных значения. Обрезаем $ \\Sigma $ до размерности $ 2 \\times 2 $, а $ U $ и $ V $ до $ 3 \\times 2 $ и $ 2 \\times 3 $ соответственно:\n",
        "\n",
        "$$\n",
        "U_k = \\begin{pmatrix}\n",
        "-0.707 & 0.000 \\\\\n",
        "-0.707 & 0.000 \\\\\n",
        "0.000 & 1.000 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\Sigma_k = \\begin{pmatrix}\n",
        "1.414 & 0.000 \\\\\n",
        "0.000 & 1.000 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "V^T_k = \\begin{pmatrix}\n",
        "-0.707 & -0.707 & 0.000 \\\\\n",
        "0.707 & -0.707 & 0.000 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "**Шаг 4: Интерпретация результатов**\n",
        "\n",
        "Новая матрица термин-документ после снижения размерности будет:\n",
        "\n",
        "$$\n",
        "A_k = U_k \\Sigma_k V^T_k\n",
        "$$\n",
        "\n",
        "$$\n",
        "A_k = \\begin{pmatrix}\n",
        "-0.707 & 0.000 \\\\\n",
        "-0.707 & 0.000 \\\\\n",
        "0.000 & 1.000 \\\\\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "1.414 & 0.000 \\\\\n",
        "0.000 & 1.000 \\\\\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "-0.707 & -0.707 & 0.000 \\\\\n",
        "0.707 & -0.707 & 0.000 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Умножаем эти матрицы:\n",
        "\n",
        "$$\n",
        "A_k = \\begin{pmatrix}\n",
        "-0.707 \\cdot 1.414 + 0.000 \\cdot 0.707 & -0.707 \\cdot 1.414 + 0.000 \\cdot -0.707 & 0.000 \\\\\n",
        "-0.707 \\cdot 1.414 + 0.000 \\cdot 0.707 & -0.707 \\cdot 1.414 + 0.000 \\cdot -0.707 & 0.000 \\\\\n",
        "0.000 \\cdot 1.414 + 1.000 \\cdot 0.707 & 0.000 \\cdot -0.707 + 1.000 \\cdot -0.707 & 0.000 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "A_k = \\begin{pmatrix}\n",
        "-1.000 & -1.000 & 0.000 \\\\\n",
        "-1.000 & -1.000 & 0.000 \\\\\n",
        "0.707 & -0.707 & 0.000 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Эта матрица $ A_k $ представляет документы в новом пространстве меньшей размерности, где латентные семантические связи между терминами и документами более явные. Например, первый и второй документы теперь ближе друг к другу, что отражает их семантическое сходство (\"кошка\" упоминается в обоих).\n",
        "\n",
        "Таким образом, LSA позволяет эффективно выявлять скрытые семантические связи в текстовых данных. Применение SVD и снижение размерности помогают обнаруживать латентные структуры, улучшая качество анализа текстов.\n"
      ],
      "metadata": {
        "id": "FKD5FCdEAJSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stop-words\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Пример коллекции текстов (новостные заголовки)\n",
        "documents = [\n",
        "    \"Турция выиграла матч в чемпионате мира\",\n",
        "    \"Россия проиграла в полуфинале чемпионата Европы\",\n",
        "    \"США победили в Олимпийских играх\",\n",
        "    \"Франция вышла в финал Лиги наций\",\n",
        "    \"Италия выиграла золото в чемпионате мира\"\n",
        "]\n",
        "\n",
        "# Получение списка стоп-слов на русском языке\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "stop_words = list(ENGLISH_STOP_WORDS) + ['и', 'в', 'на', 'не', 'с', 'от', 'для', 'с', 'по', 'из', 'что', 'так', 'как', 'это', 'этот']\n",
        "\n",
        "# Инициализируем TF-IDF векторизатор с указанием стоп-слов\n",
        "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Применяем сингулярное разложение (SVD) для уменьшения размерности до 2\n",
        "lsa = TruncatedSVD(n_components=2)\n",
        "X_reduced = lsa.fit_transform(X)\n",
        "\n",
        "# Выводим результаты\n",
        "print(\"Original TF-IDF matrix shape:\", X.shape)\n",
        "print(\"Reduced matrix shape:\", X_reduced.shape)\n",
        "print(\"Singular values:\", lsa.singular_values_)\n",
        "print()\n",
        "\n",
        "# Выводим топ слов для каждой компоненты\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "for i, component in enumerate(lsa.components_):\n",
        "    top_terms = [terms[ind] for ind in component.argsort()[:-6:-1]]\n",
        "    print(f\"Top terms for component {i+1}: {', '.join(top_terms)}\")\n",
        "    print()\n",
        "\n",
        "# Выводим преобразованные документы (документы в новом пространстве)\n",
        "print(\"Transformed documents (in reduced space):\")\n",
        "for i, text in enumerate(documents):\n",
        "    print(f\"Document {i+1}: {X_reduced[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fx9MMrGXC1s3",
        "outputId": "9d2886b2-5171-4421-f338-c024c02c626c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stop-words in /usr/local/lib/python3.10/dist-packages (2018.7.23)\n",
            "Original TF-IDF matrix shape: (5, 21)\n",
            "Reduced matrix shape: (5, 2)\n",
            "Singular values: [1.2223023 1.       ]\n",
            "\n",
            "Top terms for component 1: выиграла, мира, чемпионате, золото, италия\n",
            "\n",
            "Top terms for component 2: россия, проиграла, полуфинале, чемпионата, европы\n",
            "\n",
            "Transformed documents (in reduced space):\n",
            "Document 1: [8.64298248e-01 8.20954029e-17]\n",
            "Document 2: [2.5419027e-16 9.6969697e-01]\n",
            "Document 3: [-1.31004070e-16 -2.12121212e-01]\n",
            "Document 4: [6.43022749e-17 1.21212121e-01]\n",
            "Document 5: [ 8.64298248e-01 -3.20524508e-16]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Latent Dirichlet Allocation (LDA)\n",
        "\n",
        "Latent Dirichlet Allocation (LDA) — это вероятностная модель, используемая для обнаружения скрытых тем в коллекции текстовых документов. LDA представляет документы как смеси различных тем, где каждая тема характеризуется распределением терминов. Этот метод широко применяется в задачах тематического моделирования, текстовой классификации и информационного поиска.\n",
        "\n",
        "#### Основные понятия и предположения LDA\n",
        "\n",
        "1. **Темы**: Каждая тема — это распределение вероятностей на словарных терминах.\n",
        "2. **Документы**: Каждый документ рассматривается как смесь тем, и каждая тема в документе имеет свою долю.\n",
        "3. **Слова**: Каждое слово в документе связано с одной из тем.\n",
        "\n",
        "#### Графическая модель LDA\n",
        "\n",
        "LDA можно представить в виде графической модели, где:\n",
        "- $\\alpha$ — гиперпараметр, управляющий распределением тем в документах.\n",
        "- $\\beta$ — гиперпараметр, управляющий распределением слов в темах.\n",
        "- $\\theta_d$ — распределение тем для документа $d$.\n",
        "- $\\phi_k$ — распределение слов для темы $k$.\n",
        "- $z_{d,n}$ — тема для $n$-го слова в документе $d$.\n",
        "- $w_{d,n}$ — $n$-ое слово в документе $d$.\n",
        "\n",
        "Графическая модель для одного документа выглядит следующим образом:\n",
        "\n",
        "$$\n",
        "\\begin{array}{ccccccc}\n",
        "\\alpha & & \\beta & & & & \\\\\n",
        "& \\theta_d & & \\phi_k & & & \\\\\n",
        "& \\downarrow & & \\downarrow & & & \\\\\n",
        "\\text{(Dir)} & & \\text{(Dir)} & & & & \\\\\n",
        "& & z_{d,n} & \\rightarrow & w_{d,n} & & \\\\\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "#### Основные шаги LDA\n",
        "\n",
        "1. **Генерация распределений тем для каждого документа**:\n",
        "   - Для каждого документа $d$ выбирается распределение тем $\\theta_d \\sim \\text{Dir}(\\alpha)$.\n",
        "\n",
        "2. **Генерация распределений слов для каждой темы**:\n",
        "   - Для каждой темы $k$ выбирается распределение слов $\\phi_k \\sim \\text{Dir}(\\beta)$.\n",
        "\n",
        "3. **Генерация слов для документов**:\n",
        "   - Для каждого слова $n$ в документе $d$:\n",
        "     - Выбирается тема $z_{d,n} \\sim \\text{Multinomial}(\\theta_d)$.\n",
        "     - Выбирается слово $w_{d,n}$ согласно распределению слов для темы $z_{d,n}$, то есть $w_{d,n} \\sim \\text{Multinomial}(\\phi_{z_{d,n}})$.\n",
        "\n",
        "#### Формулы LDA\n",
        "\n",
        "1. **Вероятность темы $z_{d,n}$ для слова $w_{d,n}$**:\n",
        "   $$\n",
        "   P(z_{d,n} = k | w_{d,n} = v, \\theta_d, \\phi_k) \\propto P(w_{d,n} = v | z_{d,n} = k, \\phi_k) P(z_{d,n} = k | \\theta_d)\n",
        "   $$\n",
        "\n",
        "2. **Совместная вероятность модели**:\n",
        "   $$\n",
        "   P(w, z | \\alpha, \\beta) = \\prod_{d=1}^D \\left[ P(\\theta_d | \\alpha) \\prod_{n=1}^{N_d} P(z_{d,n} | \\theta_d) P(w_{d,n} | z_{d,n}, \\phi) \\right]\n",
        "   $$\n",
        "\n",
        "3. **Маргинальная вероятность документа $d$**:\n",
        "   $$\n",
        "   P(w_d | \\alpha, \\beta) = \\int P(\\theta_d | \\alpha) \\left( \\prod_{n=1}^{N_d} \\sum_{z_{d,n}} P(z_{d,n} | \\theta_d) P(w_{d,n} | z_{d,n}, \\beta) \\right) d\\theta_d\n",
        "   $$\n",
        "\n",
        "#### Пример работы LDA\n",
        "\n",
        "Рассмотрим пример с тремя документами и двумя темами.\n",
        "\n",
        "1. **Документы**:\n",
        "   - Документ 1: \"Кошка играет с мячом\"\n",
        "   - Документ 2: \"Собака любит бегать\"\n",
        "   - Документ 3: \"Кошка и собака играют вместе\"\n",
        "\n",
        "2. **Предположим две темы**:\n",
        "   - Тема 1: Домашние животные\n",
        "   - Тема 2: Игры и активность\n",
        "\n",
        "3. **Процесс генерации**:\n",
        "\n",
        "   - **Шаг 1**: Определим распределение тем для каждого документа ($\\theta$):\n",
        "     - Документ 1: $\\theta_1 = [0.6, 0.4]$ (60% о домашних животных, 40% об играх)\n",
        "     - Документ 2: $\\theta_2 = [0.3, 0.7]$ (30% о домашних животных, 70% об играх)\n",
        "     - Документ 3: $\\theta_3 = [0.5, 0.5]$ (50% о домашних животных, 50% об играх)\n",
        "\n",
        "   - **Шаг 2**: Определим распределение слов для каждой темы ($\\phi$):\n",
        "     - Тема 1: $\\phi_1 = [0.3, 0.3, 0.2, 0.2]$ (слова: \"кошка\", \"собака\", \"играет\", \"бегает\")\n",
        "     - Тема 2: $\\phi_2 = [0.1, 0.1, 0.4, 0.4]$ (слова: \"кошка\", \"собака\", \"играет\", \"бегает\")\n",
        "\n",
        "   - **Шаг 3**: Генерация слов для каждого документа:\n",
        "     - Для документа 1:\n",
        "       - Выбираем тему для первого слова, например, Тема 1.\n",
        "       - Выбираем слово из Тема 1, например, \"кошка\".\n",
        "       - Повторяем процесс для всех слов в документе.\n",
        "\n",
        "#### Оценка параметров в LDA\n",
        "\n",
        "Для оценки параметров $\\theta$ и $\\phi$ используются различные методы, включая вариационный байесовский вывод и метод Гиббсовой выборки.\n",
        "\n",
        "1. **Гиббсовая выборка**:\n",
        "   - Итеративный метод для аппроксимации распределений $\\theta$ и $\\phi$.\n",
        "   - Обновляются условные распределения $P(z_{d,n} | w, z_{-d,n})$, где $z_{-d,n}$ — это все переменные $z$ кроме $z_{d,n}$.\n",
        "\n",
        "2. **Вариационный байесовский вывод**:\n",
        "   - Метод оптимизации для аппроксимации распределений $\\theta$ и $\\phi$.\n",
        "   - Заменяет сложные распределения на более простые аппроксимации и минимизирует расстояние между ними.\n",
        "\n",
        "#### Применение LDA\n",
        "\n",
        "LDA широко применяется в различных областях, таких как:\n",
        "\n",
        "1. **Тематическое моделирование**:\n",
        "   - Обнаружение тем в больших коллекциях текстов, таких как научные статьи, новости, блоги.\n",
        "\n",
        "2. **Текстовая классификация**:\n",
        "   - Автоматическая классификация документов по темам.\n",
        "\n",
        "3. **Информационный поиск**:\n",
        "   - Улучшение релевантности поиска путем учета скрытых тем в документах.\n",
        "\n",
        "4. **Рекомендательные системы**:\n",
        "   - Рекомендация контента пользователям на основе их интересов, определенных через темы.\n",
        "\n"
      ],
      "metadata": {
        "id": "uIPoiWweAefB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Конкретный пример работы LDA\n",
        "\n",
        "Рассмотрим пример с тремя документами и двумя темами.\n",
        "\n",
        "##### Документы:\n",
        "- Документ 1: \"Кошка играет с мячом\"\n",
        "- Документ 2: \"Собака любит бегать\"\n",
        "- Документ 3: \"Кошка и собака играют вместе\"\n",
        "\n",
        "##### Шаг 1: Построение матрицы термин-документ\n",
        "\n",
        "Предположим, у нас есть следующие термины: \"кошка\", \"играет\", \"мяч\", \"собака\", \"любит\", \"бегать\", \"вместе\".\n",
        "\n",
        "Матрица термин-документ:\n",
        "\n",
        "$$\n",
        "\\begin{array}{c|ccc}\n",
        " & \\text{Документ 1} & \\text{Документ 2} & \\text{Документ 3} \\\\\n",
        "\\hline\n",
        "\\text{кошка} & 1 & 0 & 1 \\\\\n",
        "\\text{играет} & 1 & 0 & 1 \\\\\n",
        "\\text{мяч} & 1 & 0 & 0 \\\\\n",
        "\\text{собака} & 0 & 1 & 1 \\\\\n",
        "\\text{любит} & 0 & 1 & 0 \\\\\n",
        "\\text{бегать} & 0 & 1 & 0 \\\\\n",
        "\\text{вместе} & 0 & 0 & 1 \\\\\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "##### Шаг 2: Генерация распределений тем для каждого документа\n",
        "\n",
        "Для упрощения примера, предположим следующие значения параметров:\n",
        "\n",
        "- Гиперпараметр $\\alpha$ для распределения тем: $\\alpha = [0.5, 0.5]$\n",
        "- Гиперпараметр $\\beta$ для распределения слов: $\\beta = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]$\n",
        "\n",
        "Распределение тем для каждого документа ($\\theta$) может быть следующим:\n",
        "\n",
        "- Документ 1: $\\theta_1 = [0.6, 0.4]$ (60% Тема 1, 40% Тема 2)\n",
        "- Документ 2: $\\theta_2 = [0.3, 0.7]$ (30% Тема 1, 70% Тема 2)\n",
        "- Документ 3: $\\theta_3 = [0.5, 0.5]$ (50% Тема 1, 50% Тема 2)\n",
        "\n",
        "##### Шаг 3: Генерация распределений слов для каждой темы\n",
        "\n",
        "Распределение слов для каждой темы ($\\phi$) может быть следующим:\n",
        "\n",
        "- Тема 1: $\\phi_1 = [0.3, 0.3, 0.2, 0.1, 0.05, 0.025, 0.025]$\n",
        "- Тема 2: $\\phi_2 = [0.1, 0.1, 0.1, 0.4, 0.1, 0.1, 0.1]$\n",
        "\n",
        "##### Шаг 4: Генерация слов для документов\n",
        "\n",
        "Для каждого слова $n$ в документе $d$:\n",
        "\n",
        "- Выбираем тему $z_{d,n}$ из распределения тем $\\theta_d$.\n",
        "- Выбираем слово $w_{d,n}$ из распределения слов $\\phi_{z_{d,n}}$.\n",
        "\n",
        "Рассмотрим пример для Документа 1: \"Кошка играет с мячом\".\n",
        "\n",
        "1. Первое слово \"кошка\":\n",
        "   - Выбираем тему: $z_{1,1} \\sim \\text{Multinomial}(\\theta_1)$. Пусть выбрана Тема 1.\n",
        "   - Выбираем слово из Тема 1: $w_{1,1} \\sim \\text{Multinomial}(\\phi_1)$. Пусть выбрано \"кошка\".\n",
        "\n",
        "2. Второе слово \"играет\":\n",
        "   - Выбираем тему: $z_{1,2} \\sim \\text{Multinomial}(\\theta_1)$. Пусть выбрана Тема 1.\n",
        "   - Выбираем слово из Тема 1: $w_{1,2} \\sim \\text{Multinomial}(\\phi_1)$. Пусть выбрано \"играет\".\n",
        "\n",
        "3. Третье слово \"с\":\n",
        "   - Выбираем тему: $z_{1,3} \\sim \\text{Multinomial}(\\theta_1)$. Пусть выбрана Тема 2.\n",
        "   - Выбираем слово из Тема 2: $w_{1,3} \\sim \\text{Multinomial}(\\phi_2)$. Пусть выбрано \"с\".\n",
        "\n",
        "4. Четвертое слово \"мячом\":\n",
        "   - Выбираем тему: $z_{1,4} \\sim \\text{Multinomial}(\\theta_1)$. Пусть выбрана Тема 1.\n",
        "   - Выбираем слово из Тема 1: $w_{1,4} \\sim \\text{Multinomial}(\\phi_1)$. Пусть выбрано \"мячом\".\n",
        "\n",
        "#### Оценка параметров в LDA\n",
        "\n",
        "Для оценки параметров $\\theta$ и $\\phi$ используются различные методы, включая вариационный байесовский вывод и метод Гиббсовой выборки.\n",
        "\n",
        "1. **Гиббсовая выборка**:\n",
        "   - Итеративный метод для аппроксимации распределений $\\theta$ и $\\phi$.\n",
        "   - Обновляются условные распределения $P(z_{d,n} | w, z_{-d,n})$, где $z_{-d,n}$ — это все переменные $z$ кроме $z_{d,n}$.\n",
        "\n",
        "2. **Вариационный байесовский вывод**:\n",
        "   - Метод оптимизации для аппроксимации распределений $\\theta$ и $\\phi$.\n",
        "   - Заменяет сложные распределения на более простые аппроксимации и минимизирует расстояние между ними.\n",
        "\n",
        "Таким образом, Latent Dirichlet Allocation (LDA) — мощный инструмент для анализа текстов, позволяющий выявлять скрытые семантические структуры и улучшать обработку текстовых данных. Вероятностный подход и применение байесовских методов делают LDA гибким и эффективным для различных задач обработки естественного языка.\n"
      ],
      "metadata": {
        "id": "vCsVN3hfBQGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq00rpx6CByp",
        "outputId": "6c8bf78b-65b0-4312-af25-82db1bd6a7b4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.11.4)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.0.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.4)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.10.0)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (2.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (1.14.1)\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-2.0 pyLDAvis-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Скачиваем ресурсы NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Пример документов\n",
        "documents = [\n",
        "    \"Кошка играет с мячом\",\n",
        "    \"Собака любит бегать\",\n",
        "    \"Кошка и собака играют вместе\"\n",
        "]\n",
        "\n",
        "# Предобработка текстов\n",
        "stop_words = set(stopwords.words('russian'))\n",
        "texts = [\n",
        "    [word for word in word_tokenize(document.lower()) if word.isalnum() and word not in stop_words]\n",
        "    for document in documents\n",
        "]\n",
        "\n",
        "# Создание словаря и корпуса\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Параметры LDA\n",
        "num_topics = 2  # Количество тем\n",
        "passes = 10     # Количество проходов для улучшения модели\n",
        "\n",
        "# Обучение модели LDA\n",
        "lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=passes)\n",
        "\n",
        "# Вывод тем\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(f\"Тема {idx+1}: {topic}\")\n",
        "\n",
        "# Визуализация тем с pyLDAvis\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "\n",
        "# Визуализация LDA модели\n",
        "lda_display = gensimvis.prepare(lda_model, corpus, dictionary, sort_topics=False)\n",
        "\n",
        "# Сохранение визуализации в файл HTML\n",
        "pyLDAvis.save_html(lda_display, 'lda_vis.html')\n",
        "\n",
        "# Сообщение для пользователя\n",
        "print(\"Визуализация сохранена в файл lda_vis.html. Откройте его в браузере для просмотра.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xd4ql4miBzpM",
        "outputId": "db0f8a60-2610-49da-ca48-94c84ce73c81"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Тема 1: 0.219*\"кошка\" + 0.209*\"мячом\" + 0.208*\"играет\" + 0.074*\"собака\" + 0.073*\"играют\" + 0.073*\"вместе\" + 0.073*\"любит\" + 0.072*\"бегать\"\n",
            "Тема 2: 0.228*\"собака\" + 0.137*\"бегать\" + 0.137*\"любит\" + 0.137*\"вместе\" + 0.136*\"играют\" + 0.132*\"кошка\" + 0.047*\"играет\" + 0.047*\"мячом\"\n",
            "Визуализация сохранена в файл lda_vis.html. Откройте его в браузере для просмотра.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Семантический поиск на основе сходства векторов тем\n",
        "\n",
        "**2.1 Понятие семантического сходства**\n",
        "\n",
        "Семантическое сходство оценивается путем измерения расстояния или угла между векторами слов или документов. Наиболее распространенными метриками являются косинусное сходство, евклидово расстояние и расстояние Минковского.\n",
        "\n",
        "**2.2 Алгоритмы семантического поиска**\n",
        "\n",
        "1. **Поиск на основе векторов слов**: Использует сходство векторов для нахождения схожих документов или предложений.\n",
        "2. **Поиск на основе векторов предложений или документов**: Применение более сложных моделей, таких как BERT или Sentence-BERT, для получения эмбеддингов целых предложений или документов и последующего поиска по сходству.\n",
        "\n"
      ],
      "metadata": {
        "id": "b_OJdAyW_Nv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kHsJJPDd_QWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3 Применение в реальных системах**\n",
        "\n",
        "1. **Информационный поиск**: Использование семантического анализа для улучшения поиска в поисковых системах.\n",
        "2. **Рекомендательные системы**: Сравнение предпочтений пользователей с векторами тем для создания персонализированных рекомендаций.\n",
        "\n"
      ],
      "metadata": {
        "id": "OewekrVu_QZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "m2famKkQ_TY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Масштабируемый семантический анализ и семантический поиск для больших корпусов текстов\n",
        "\n",
        "**3.1 Масштабируемость моделей**\n",
        "\n",
        "Для обработки больших объемов данных требуются эффективные алгоритмы и распределенные системы. Использование параллельных вычислений и распределенных хранилищ данных (например, Hadoop, Spark) позволяет обрабатывать огромные текстовые корпусы.\n",
        "\n",
        "**3.2 Инструменты и технологии**\n",
        "\n",
        "1. **Apache Spark**: Платформа для распределенной обработки данных, обеспечивающая быстрый и масштабируемый анализ.\n",
        "2. **Distributed Machine Learning**: Использование распределенных версий алгоритмов машинного обучения, таких как распределенные реализации Word2Vec и GloVe.\n",
        "\n"
      ],
      "metadata": {
        "id": "fyPlk5JE_Tb3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "su_U3Kh2_WD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3 Обработка больших данных**\n",
        "\n",
        "Для работы с большими корпусами текстов применяются следующие подходы:\n",
        "1. **MapReduce**: Парадигма для обработки больших данных, разделяющая задачи на подзадачи и выполняющая их параллельно.\n",
        "2. **Batch Processing и Stream Processing**: Batch Processing для анализа больших объемов данных за раз и Stream Processing для обработки данных в реальном времени.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZixpQOK9_WHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "guD4Y1r1_Yqx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Семантические компоненты (темы) как признаки в конвейере NLP\n",
        "\n",
        "**4.1 Интеграция семантических компонентов**\n",
        "\n",
        "Векторные представления тем могут быть использованы в качестве признаков в различных NLP-задачах, таких как классификация текстов, кластеризация, распознавание сущностей и др.\n",
        "\n",
        "**4.2 Примеры применения**\n",
        "\n",
        "1. **Классификация текстов**: Использование векторов тем для обучения моделей классификации, например, для определения тематики документа.\n",
        "2. **Кластеризация документов**: Группировка документов по семантической схожести с использованием векторных представлений.\n",
        "\n"
      ],
      "metadata": {
        "id": "HpsBJNVv_Ytw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A61Ojzsv_bDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.3 Построение конвейеров NLP**\n",
        "\n",
        "1. **Feature Extraction**: Извлечение векторных признаков из текста.\n",
        "2. **Model Training**: Обучение моделей на основе этих признаков.\n",
        "3. **Model Evaluation**: Оценка качества моделей с использованием метрик, таких как точность, полнота и F-мера.\n",
        "\n"
      ],
      "metadata": {
        "id": "LLQvqMel_bGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2CwymDaX_dW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Ориентация в векторных пространствах высокой размерности\n",
        "\n",
        "**5.1 Высокая размерность и проблема курсовых**\n",
        "\n",
        "Проблема курсовых возникает при работе в пространствах высокой размерности, где расстояния между точками становятся менее информативными.\n",
        "\n",
        "**5.2 Методы уменьшения размерности**\n",
        "\n",
        "Для работы в высокоразмерных пространствах часто используются методы уменьшения размерности:\n",
        "1. **Principal Component Analysis (PCA)**: Преобразует данные в меньшее число измерений, сохраняя как можно больше вариации данных.\n",
        "2. **t-SNE**: Метод для визуализации высокоразмерных данных, сохраняющий локальную структуру данных.\n",
        "3. **UMAP**: Современный метод уменьшения размерности, который может быть более эффективным для некоторых задач, чем t-SNE.\n",
        "\n"
      ],
      "metadata": {
        "id": "imNbmyJf_dZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "D0MTnh9P_fcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.3 Практическое применение**\n",
        "\n",
        "1. **Визуализация данных**: Использование методов уменьшения размерности для визуализации и анализа векторов слов и тем.\n",
        "2. **Повышение эффективности алгоритмов**: Уменьшение размерности для ускорения работы алгоритмов машинного обучения и снижения потребления памяти.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qwScX7cq_ffT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dt8Spz8m_kqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LaOJ9q7u_ksz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Таким, образом, семантический анализ и векторное представление слов и тем являются фундаментальными компонентами современных систем NLP. Они позволяют создавать мощные и масштабируемые решения для поиска и анализа текстов. Понимание этих методов и их правильное применение открывает широкие возможности для разработки инновационных приложений и сервисов в области обработки естественного языка."
      ],
      "metadata": {
        "id": "1CxB3IUf_kv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Dw0iGo4S-mb1"
      }
    }
  ]
}