{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP87AhcPeu+3gFJ2JhPS4vM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/NLP-2024-2025/blob/main/Lecture_4_%D0%9A%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F_%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Лекция: Классификация текста\n",
        "\n",
        "### Введение\n",
        "\n",
        "Классификация текста является важной задачей в области обработки естественного языка (NLP). Она находит применение в спам-фильтрах, анализе тональности, категоризации новостей и многих других областях. В данной лекции мы рассмотрим основные методы машинного обучения, используемые для классификации текстов, включая методы предварительной обработки данных, алгоритмы классификации и оценку их эффективности.\n",
        "\n",
        "### 1. Предварительная обработка данных\n",
        "\n",
        "Перед тем как применять методы машинного обучения к тексту, необходимо выполнить несколько шагов предварительной обработки данных.\n",
        "\n",
        "#### 1.1. Токенизация\n",
        "\n",
        "Токенизация — это процесс разбиения текста на отдельные элементы (токены), такие как слова или фразы. Это можно сделать с помощью регулярных выражений или специализированных библиотек, таких как NLTK или SpaCy.\n"
      ],
      "metadata": {
        "id": "jrDZG7aye41q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Machine learning is fascinating.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97cO3heNfAlv",
        "outputId": "4b85cf02-07bb-4019-cdec-31d73ad665b7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Machine', 'learning', 'is', 'fascinating', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 1.2. Приведение к нижнему регистру\n",
        "\n",
        "Приведение всех символов к нижнему регистру помогает уменьшить разнообразие слов и повысить точность модели.\n"
      ],
      "metadata": {
        "id": "BLBvCMAefFFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.lower()\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "huzqvWavfbb0",
        "outputId": "27f1e5ac-ffb0-45f6-e41f-d18cf2107baf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'machine learning is fascinating.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3. Удаление стоп-слов\n",
        "\n",
        "Стоп-слова (например, \"и\", \"на\", \"в\") часто не несут значимой информации и могут быть удалены.\n"
      ],
      "metadata": {
        "id": "OfcVf9hffj8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [word for word in tokens if word not in stop_words]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDMofktAfkj2",
        "outputId": "8ba390eb-0cd5-44c4-c3f0-ecbab6d5658f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4. Лемматизация и стемминг\n",
        "\n",
        "Лемматизация и стемминг используются для приведения слов к их базовым формам.\n"
      ],
      "metadata": {
        "id": "R5rOEPnUf1Pz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grts6BETf4J3",
        "outputId": "225ba069-90ef-4d34-9334-5b9e3323bf01"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Machine', 'learning', 'fascinating', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Представление текста\n",
        "\n",
        "#### 2.1. Мешок слов (Bag of Words, BoW)\n",
        "\n",
        "Модель \"мешок слов\" представляет текст в виде вектора частот слов. Для этого используется векторизация, где каждая позиция вектора соответствует частоте появления определенного слова в документе.\n"
      ],
      "metadata": {
        "id": "8vfHk0sUgfXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\"Machine learning is fascinating.\", \"Text classification with machine learning.\"]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qqBzW77ggD7",
        "outputId": "47ada3a9-e3fb-4b10-ba4d-2fc0a6795cd7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 1 1 1 0 0]\n",
            " [1 0 0 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 2.2. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "\n",
        "TF-IDF улучшает модель BoW, взвешивая частоты слов на основе их важности.\n",
        "\n",
        "Формула TF-IDF для слова $ t $ в документе $ d $:\n",
        "\n",
        "$$\n",
        "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
        "$$\n",
        "\n",
        "где\n",
        "\n",
        "$$\n",
        "\\text{TF}(t, d) = \\frac{\\text{Количество вхождений } t \\text{ в } d}{\\text{Общее количество слов в } d}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{IDF}(t) = \\log \\left( \\frac{\\text{Количество документов}}{\\text{Количество документов, содержащих } t} \\right)\n",
        "$$\n"
      ],
      "metadata": {
        "id": "n4ubNIIcgsKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilrbmCZGg_Ql",
        "outputId": "dae98e1e-24c9-4b7c-fbf8-88e5c10f3898"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.57615236 0.57615236 0.40993715 0.40993715 0.\n",
            "  0.        ]\n",
            " [0.49922133 0.         0.         0.35520009 0.35520009 0.49922133\n",
            "  0.49922133]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Алгоритмы классификации\n",
        "\n",
        "Классификация - это одна из основных задач машинного обучения, где цель состоит в том, чтобы определить, к какому классу или категории принадлежит объект на основе его признаков.\n",
        "\n",
        "Некоторые наиболее популярные алгоритмы классификации включают:\n",
        "\n",
        "1. **Наивный байесовский классификатор (Naive Bayes)**: Это вероятностный классификатор, основанный на теореме Байеса, который предполагает, что признаки независимы.\n",
        "\n",
        "2. **Логистическая регрессия (Logistic Regression)**: Это алгоритм, использующий регрессионную модель для прогнозирования категориального (бинарного или многоклассового) выходного значения.\n",
        "\n",
        "3. **Метод опорных векторов (Support Vector Machines, SVM)**: Это алгоритм, который строит гиперплоскость или набор гиперплоскостей в многомерном пространстве, которые могут использоваться для классификации.\n",
        "\n",
        "\n",
        "4. **K-ближайших соседей (K-Nearest Neighbors, KNN)**: Это алгоритм, который классифицирует объект на основе большинства голосов его K ближайших соседей в пространстве признаков.\n",
        "\n",
        "4. **Деревья решений (Decision Trees)**: Это алгоритм, который создает модель принятия решений в виде дерева с решениями и их возможными последствиями.\n",
        "\n",
        "5. **Случайный лес (Random Forest)**: Это ансамблевый алгоритм, который использует множество деревьев решений для более точного прогнозирования.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Каждый из этих алгоритмов имеет свои сильные и слабые стороны, и выбор наиболее подходящего алгоритма зависит от характера данных, требуемой точности и интерпретируемости модели."
      ],
      "metadata": {
        "id": "WMQPAsbohFvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Наивный байесовский классификатор (Naive Bayes):"
      ],
      "metadata": {
        "id": "KbAafh_nhv8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Введение\n",
        "\n",
        "Наивный байесовский классификатор (Наивный Байес) — это один из самых простых и эффективных алгоритмов для задач классификации в машинном обучении. Он основывается на теореме Байеса с предположением о независимости признаков. В этой лекции мы подробно рассмотрим, как работает Наивный Байесовский классификатор в контексте обработки естественного языка (NLP), обсудим теоретическую основу, а также приведем примеры и формулы.\n",
        "\n",
        "#### Теоретическая основа\n",
        "\n",
        "##### Теорема Байеса\n",
        "\n",
        "Теорема Байеса описывает вероятность гипотезы $H$ при условии наблюдаемого события $E$:\n",
        "\n",
        "$$ P(H|E) = \\frac{P(E|H) \\cdot P(H)}{P(E)} $$\n",
        "\n",
        "где:\n",
        "- $P(H|E)$ — апостериорная вероятность гипотезы $H$ при условии наблюдения $E$.\n",
        "- $P(E|H)$ — вероятность наблюдения $E$ при условии, что гипотеза $H$ истинна.\n",
        "- $P(H)$ — априорная вероятность гипотезы $H$.\n",
        "- $P(E)$ — вероятность наблюдения $E$.\n",
        "\n",
        "##### Наивное предположение о независимости\n",
        "\n",
        "Наивный Байесовский классификатор предполагает, что все признаки (слова в тексте) независимы друг от друга. Хотя это предположение редко выполняется в реальной жизни, оно упрощает вычисления и часто даёт хорошие результаты.\n",
        "\n",
        "#### Применение в NLP\n",
        "\n",
        "##### Вероятность текста\n",
        "\n",
        "Если предположить, что слова в тексте независимы, вероятность текста $d = \\{w_1, w_2, \\ldots, w_n\\}$ при условии класса $C_i$ равна:\n",
        "\n",
        "$$ P(d|C_i) = P(w_1, w_2, \\ldots, w_n|C_i) = P(w_1|C_i) \\cdot P(w_2|C_i) \\cdot \\ldots \\cdot P(w_n|C_i) $$\n",
        "\n",
        "Таким образом, вероятность $P(C_i|d)$ можно записать как:\n",
        "\n",
        "$$ P(C_i|d) \\propto P(C_i) \\cdot \\prod_{j=1}^{n} P(w_j|C_i) $$\n",
        "\n",
        "##### Оценка вероятностей\n",
        "\n",
        "Априорная вероятность класса $P(C_i)$ оценивается как доля документов этого класса в обучающей выборке:\n",
        "\n",
        "$$ P(C_i) = \\frac{\\text{количество документов класса } C_i}{\\text{общее количество документов}} $$\n",
        "\n",
        "Условная вероятность слова $w_j$ при условии класса $C_i$ оценивается как:\n",
        "\n",
        "$$ P(w_j|C_i) = \\frac{\\text{количество вхождений } w_j \\text{ в документы класса } C_i + 1}{\\text{общее количество слов в документах класса } C_i + V} $$\n",
        "\n",
        "где $V$ — размер словаря (общее количество уникальных слов). Добавление единицы (Лапласов сглаживатель) предотвращает вероятность нуля для слов, которые отсутствуют в документах класса.\n",
        "\n",
        "#### Пример\n",
        "\n",
        "Рассмотрим простой пример классификации текста на спам и не спам.\n",
        "\n",
        "**Обучающая выборка:**\n",
        "\n",
        "- Документ 1 (спам): \"купить дешево\"\n",
        "- Документ 2 (не спам): \"хорошее качество\"\n",
        "- Документ 3 (спам): \"купить быстро дешево\"\n",
        "- Документ 4 (не спам): \"качество гарантировано\"\n",
        "\n",
        "Словарь: \"купить\", \"дешево\", \"хорошее\", \"качество\", \"быстро\", \"гарантировано\".\n",
        "\n",
        "**Оценка априорных вероятностей:**\n",
        "\n",
        "$$ P(\\text{спам}) = \\frac{2}{4} = 0.5 $$\n",
        "$$ P(\\text{не спам}) = \\frac{2}{4} = 0.5 $$\n",
        "\n",
        "**Оценка условных вероятностей:**\n",
        "\n",
        "Для класса \"спам\":\n",
        "\n",
        "$$ P(\\text{купить}|\\text{спам}) = \\frac{2 + 1}{5 + 6} = \\frac{3}{11} $$\n",
        "$$ P(\\text{дешево}|\\text{спам}) = \\frac{2 + 1}{5 + 6} = \\frac{3}{11} $$\n",
        "$$ P(\\text{быстро}|\\text{спам}) = \\frac{1 + 1}{5 + 6} = \\frac{2}{11} $$\n",
        "\n",
        "Для класса \"не спам\":\n",
        "\n",
        "$$ P(\\text{хорошее}|\\text{не спам}) = \\frac{1 + 1}{4 + 6} = \\frac{2}{10} = 0.2 $$\n",
        "$$ P(\\text{качество}|\\text{не спам}) = \\frac{2 + 1}{4 + 6} = \\frac{3}{10} = 0.3 $$\n",
        "$$ P(\\text{гарантировано}|\\text{не спам}) = \\frac{1 + 1}{4 + 6} = \\frac{2}{10} = 0.2 $$\n",
        "\n",
        "**Классификация нового документа:**\n",
        "\n",
        "Документ: \"купить качество\"\n",
        "\n",
        "Вычисляем вероятность для класса \"спам\":\n",
        "\n",
        "$$ P(\\text{спам}|\\text{купить качество}) \\propto P(\\text{спам}) \\cdot P(\\text{купить}|\\text{спам}) \\cdot P(\\text{качество}|\\text{спам}) $$\n",
        "$$ P(\\text{спам}|\\text{купить качество}) \\propto 0.5 \\cdot \\frac{3}{11} \\cdot \\frac{1}{11} = \\frac{3}{242} \\approx 0.0124 $$\n",
        "\n",
        "Вычисляем вероятность для класса \"не спам\":\n",
        "\n",
        "$$ P(\\text{не спам}|\\text{купить качество}) \\propto P(\\text{не спам}) \\cdot P(\\text{купить}|\\text{не спам}) \\cdot P(\\text{качество}|\\text{не спам}) $$\n",
        "$$ P(\\text{не спам}|\\text{купить качество}) \\propto 0.5 \\cdot \\frac{1}{10} \\cdot 0.3 = 0.015 $$\n",
        "\n",
        "Так как $0.015 > 0.0124$, документ \"купить качество\" классифицируется как \"не спам\".\n",
        "\n",
        "Таким образом, наивный Байесовский классификатор — мощный инструмент для задач классификации текста, несмотря на своё наивное предположение о независимости признаков. Его простота и эффектив"
      ],
      "metadata": {
        "id": "T1jCkqIiwnvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ниже приведены примеры использования Наивного Байесовского классификатора для классификации текста с использованием библиотеки scikit-learn в Python."
      ],
      "metadata": {
        "id": "rWHUiP-Xzqq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Пример данных\n",
        "data = {\n",
        "    'text': ['купить дешево', 'хорошее качество', 'купить быстро дешево', 'качество гарантировано',\n",
        "             'дешево и быстро', 'отличное качество', 'купить сразу', 'качество на высоте'],\n",
        "    'label': ['спам', 'не спам', 'спам', 'не спам', 'спам', 'не спам', 'спам', 'не спам']\n",
        "}\n",
        "\n",
        "# Создаем DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Разделение на обучающую и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "glbrcq2czrd1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Преобразование текстов в числовые признаки\n",
        "Для использования Наивного Байесовского классификатора, нужно преобразовать тексты в числовые признаки с помощью CountVectorizer."
      ],
      "metadata": {
        "id": "iSX1cRUTz0ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer это инструмент машинного обучения, который используется для преобразования текстовых данных в числовые векторные представления. Он подсчитывает частоту появления каждого слова (или n-граммы) в документе и создает разреженную матрицу, где каждая строка соответствует документу, а каждый столбец - уникальному слову (или n-грамме) в корпусе.\n",
        "\n",
        "Основные шаги работы CountVectorizer:\n",
        "\n",
        "1. Разбиение текста на токены (слова или n-граммы).\n",
        "2. Построение словаря уникальных токенов.\n",
        "3. Подсчет частоты появления каждого токена в каждом документе.\n",
        "4. Формирование разреженной матрицы, где каждая строка - документ, а каждый столбец - токен.\n",
        "\n",
        "Преимущества CountVectorizer:\n",
        "- Простота и эффективность.\n",
        "- Возможность учитывать n-граммы, а не только отдельные слова.\n",
        "- Возможность настройки параметров (min_df, max_df, ngram_range и др.).\n",
        "- Интеграция с другими инструментами машинного обучения в библиотеке scikit-learn.\n",
        "\n",
        "Пример использования в Python:\n"
      ],
      "metadata": {
        "id": "sXFolJLJ0Amn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Создание объекта CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Преобразование текстовых данных в матрицу частот\n",
        "X = vectorizer.fit_transform([\"Это первый документ.\", \"Это второй документ.\"])\n",
        "\n",
        "# Получение матрицы\n",
        "print(X.toarray())\n",
        "\n",
        "# Получение словаря\n",
        "print(vectorizer.vocabulary_)"
      ],
      "metadata": {
        "id": "6YE4m6o40D6t",
        "outputId": "95379e38-350e-429d-c66f-9d18082db0d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 1 1]\n",
            " [1 1 0 1]]\n",
            "{'это': 3, 'первый': 2, 'документ': 1, 'второй': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Преобразование текста в вектор признаков\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "1hsvDdkX0KWW"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Обучение модели**\n",
        "\n",
        "Создадим и обучим модель Наивного Байесовского классификатора."
      ],
      "metadata": {
        "id": "Xp9jDCMS0Rk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание и обучение модели\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_vec, y_train)"
      ],
      "metadata": {
        "id": "TytnUHEM0V0C",
        "outputId": "1ab90144-ebbd-4194-c917-e281b2edf7bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оценка модели\n",
        "\n",
        "Оценим качество работы модели на тестовой выборке."
      ],
      "metadata": {
        "id": "AHjWvtkw0daa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Предсказание на тестовой выборке\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "# Оценка точности\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "# Подробный отчет о классификации\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "DeyV41xl0ej3",
        "outputId": "e781dceb-ad5f-4fa1-ed83-ad42421a2197",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     не спам       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           1.00         2\n",
            "   macro avg       1.00      1.00      1.00         2\n",
            "weighted avg       1.00      1.00      1.00         2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Весь код."
      ],
      "metadata": {
        "id": "UivxigYr02Qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Пример данных\n",
        "data = {\n",
        "    'text': ['купить дешево', 'хорошее качество', 'купить быстро дешево', 'качество гарантировано',\n",
        "             'дешево и быстро', 'отличное качество', 'купить сразу', 'качество на высоте'],\n",
        "    'label': ['спам', 'не спам', 'спам', 'не спам', 'спам', 'не спам', 'спам', 'не спам']\n",
        "}\n",
        "\n",
        "# Создаем DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Разделение на обучающую и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.25, random_state=42)\n",
        "\n",
        "# Преобразование текста в вектор признаков\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Создание и обучение модели\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# Предсказание на тестовой выборке\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "# Оценка точности\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "# Подробный отчет о классификации\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "RgLMHBxb03kJ",
        "outputId": "6898dfda-97e9-479b-a133-47fec2c27a08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     не спам       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           1.00         2\n",
            "   macro avg       1.00      1.00      1.00         2\n",
            "weighted avg       1.00      1.00      1.00         2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видно модель Наивного Байеса показывает 100% точность на тестовых данных, что означает, что она безошибочно классифицирует сообщения как 'спам' или 'не спам'.\n",
        "\n",
        "Давайте рассмотрим, что это означает:\n",
        "\n",
        "1. **Точность (Accuracy)**: Значение 1.0 (или 100%) говорит о том, что все предсказания модели были правильными.\n",
        "\n",
        "2. **Отчет о классификации**:\n",
        "   - **Precision (Точность)**: Для класса 'не спам' модель имеет 100% точность, что означает, что все примеры, классифицированные как 'не спам', действительно являются 'не спам'.\n",
        "   - **Recall (Полнота)**: Модель также имеет 100% полноту для класса 'не спам', что означает, что она правильно распознала все 'не спам' примеры.\n",
        "   - **F1-score**: Гармоническое среднее между точностью и полнотой также составляет 100%, что говорит о высоком качестве классификации.\n",
        "\n",
        "Такие высокие показатели означают, что модель отлично справляется с распознаванием 'спам' и 'не спам' сообщений в данном наборе данных. Это может быть связано с тем, что данные хорошо разделены и модель Наивного Байеса эффективно выявляет отличия между ними.\n",
        "\n",
        "Теперь вы можете с уверенностью использовать эту модель для классификации новых текстовых данных. Если вы захотите улучшить ее производительность, вы можете попробовать другие методы предварительной обработки текста или алгоритмы машинного обучения. Но для этого набора данных модель Наивного Байеса показывает отличные результаты."
      ],
      "metadata": {
        "id": "kdHO1vzwoHgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для использования результатов классификации можно выполнить следующие шаги:\n",
        "\n",
        "1. Получение предсказаний: После обучения модели, вы можете использовать метод predict() для получения предсказаний на новых данных. Например:\n",
        "\n"
      ],
      "metadata": {
        "id": "2MH7_i_nnuc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = ['купить срочно дешево', 'качество супер']\n",
        "new_vec = vectorizer.transform(new_text)\n",
        "new_predictions = model.predict(new_vec)\n",
        "print(f'Предсказания: {new_predictions}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeoscJDgn6dw",
        "outputId": "f0c0c237-fc89-49e7-913e-565ee6b8d833"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предсказания: ['спам' 'не спам']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Принятие решений**:\n",
        "\n",
        "Например, если мы получили предсказание 'спам', мы можем автоматически перемещать сообщение в папку со спамом:\n"
      ],
      "metadata": {
        "id": "46UeKU0joS5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for pred in new_predictions:\n",
        "    if pred == 'спам':\n",
        "        print('Сообщение перемещено в папку со спамом')\n",
        "    else:\n",
        "        print('Сообщение добавлено в inbox')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HgfEkLdoU1_",
        "outputId": "f74d1d96-9ecf-439a-98e6-197d8b809526"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сообщение перемещено в папку со спамом\n",
            "Сообщение добавлено в inbox\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Логистическая регрессия\n",
        "\n",
        "#### Введение в логистическую регрессию\n",
        "\n",
        "Логистическая регрессия (Logistic Regression) — это статистическая модель, используемая для бинарной классификации и оценки вероятности принадлежности объекта к определённому классу. В контексте обработки естественного языка (NLP), логистическая регрессия часто применяется для решения задач, таких как определение тональности текста, классификация документов, определение категорий и тематик текстов и других задач, где необходимо определить вероятность принадлежности текста к определённому классу.\n",
        "\n",
        "#### Основные принципы логистической регрессии\n",
        "\n",
        "1. **Логистическая функция (сигмоид)**\n",
        "\n",
        "   Логистическая регрессия использует логистическую функцию (сигмоид) для преобразования выхода линейной комбинации признаков в вероятность принадлежности к классу 1 (или любому другому классу в случае многоклассовой классификации).\n",
        "\n",
        "   Формула логистической функции:\n",
        "   $$\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $$\n",
        "   где $ z = \\theta^T x $, $ \\theta $ — вектор параметров модели, $ x $ — вектор признаков.\n",
        "\n",
        "   Интерпретация $ \\sigma(z) $: $ \\sigma(z) $ представляет собой вероятность того, что объект с признаками $ x $ принадлежит классу 1.\n",
        "\n",
        "2. **Функция потерь (логистическая функция потерь)**\n",
        "\n",
        "   Чтобы обучить параметры $ \\theta $ модели логистической регрессии, используется логистическая функция потерь (log loss):\n",
        "   $$\n",
        "   J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\sigma(\\theta^T x^{(i)})) + (1 - y^{(i)}) \\log(1 - \\sigma(\\theta^T x^{(i)})) \\right]\n",
        "   $$\n",
        "   где $ m $ — количество примеров в обучающей выборке, $ y^{(i)} $ — истинная метка класса для примера $ i $.\n",
        "\n",
        "   Цель минимизации функции потерь $ J(\\theta) $ — настройка параметров $ \\theta $ для максимизации вероятности правильной классификации.\n",
        "\n",
        "3. **Градиентный спуск**\n",
        "\n",
        "   Для оптимизации параметров $ \\theta $ используется градиентный спуск:\n",
        "   $$\n",
        "   \\theta := \\theta - \\alpha \\nabla_{\\theta} J(\\theta)\n",
        "   $$\n",
        "   где $ \\alpha $ — скорость обучения (learning rate), $ \\nabla_{\\theta} J(\\theta) $ — градиент функции потерь.\n",
        "\n",
        "#### Оценка качества модели логистической регрессии в NLP\n",
        "\n",
        "1. **Метрики**\n",
        "\n",
        "   В NLP часто используются следующие метрики для оценки качества модели:\n",
        "\n",
        "   - **Точность (Accuracy)**: доля правильно классифицированных примеров.\n",
        "   $$\n",
        "   \\text{Accuracy} = \\frac{\\text{Количество правильных предсказаний}}{\\text{Общее количество предсказаний}}\n",
        "   $$\n",
        "\n",
        "   - **Точность (Precision)** и **Полнота (Recall)**: используются для более детального анализа качества модели в случае дисбаланса классов или когда важны ошибки определённого типа.\n",
        "\n",
        "   - **F1-мера (F1-score)**: гармоническое среднее точности и полноты, которое учитывает обе метрики одновременно.\n",
        "\n",
        "2. **Кросс-валидация**\n",
        "\n",
        "   Для улучшения обобщающей способности модели в NLP часто применяют кросс-валидацию. Это позволяет оценить, насколько модель способна обобщаться на новые данные.\n",
        "\n",
        "#### Примеры применения логистической регрессии в NLP\n",
        "\n",
        "- **Определение тональности текста**: классификация текстов на позитивные и негативные отзывы.\n",
        "- **Классификация текстов по темам**: определение категории статьи или документа.\n",
        "- **Анализ тональности в социальных сетях**: автоматическое определение эмоциональной окраски текстовых сообщений.\n"
      ],
      "metadata": {
        "id": "qqSOHv41n6oM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте рассмотрим задачу классификации с использованием логистической регрессии на примере. Допустим, у нас есть следующие данные:\n",
        "\n",
        "### Задача классификации\n",
        "\n",
        "Представим, что у нас есть обучающая выборка, состоящая из четырех документов, которые мы хотим классифицировать как спам или не спам.\n",
        "\n",
        "**Обучающая выборка:**\n",
        "\n",
        "- Документ 1 (спам): \"купить дешево\"\n",
        "- Документ 2 (не спам): \"хорошее качество\"\n",
        "- Документ 3 (спам): \"купить быстро дешево\"\n",
        "- Документ 4 (не спам): \"качество гарантировано\"\n",
        "\n",
        "**Словарь:** \"купить\", \"дешево\", \"хорошее\", \"качество\", \"быстро\", \"гарантировано\".\n",
        "\n",
        "### Шаг 1: Представление данных\n",
        "\n",
        "Преобразуем каждый документ в вектор признаков, используя векторное представление мешка слов на основе нашего словаря.\n",
        "\n",
        "- Документ 1 (спам): $ x^{(1)} = [1, 1, 0, 0, 0, 0] $\n",
        "- Документ 2 (не спам): $ x^{(2)} = [0, 0, 1, 1, 0, 0] $\n",
        "- Документ 3 (спам): $ x^{(3)} = [1, 1, 0, 0, 1, 0] $\n",
        "- Документ 4 (не спам): $ x^{(4)} = [0, 0, 0, 1, 0, 1] $\n",
        "\n",
        "### Шаг 2: Формулирование модели логистической регрессии\n",
        "\n",
        "Логистическая регрессия моделирует вероятность принадлежности документа к классу \"спам\" с помощью сигмоидной функции:\n",
        "\n",
        "$$ P(y = 1 | x) = \\sigma(\\theta^T x) = \\frac{1}{1 + e^{-\\theta^T x}} $$\n",
        "\n",
        "где $ \\theta $ — параметры модели, $ x $ — вектор признаков документа, $ \\sigma(z) $ — сигмоидная функция $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $.\n",
        "\n",
        "### Шаг 3: Функция потерь и обучение модели\n",
        "\n",
        "Функция потерь для логистической регрессии (логистическая функция потерь) выглядит следующим образом:\n",
        "\n",
        "$$ J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\sigma(\\theta^T x^{(i)})) + (1 - y^{(i)}) \\log(1 - \\sigma(\\theta^T x^{(i)})) \\right] $$\n",
        "\n",
        "где $ m $ — количество документов в обучающей выборке, $ y^{(i)} $ — метка класса для документа $ i $.\n",
        "\n",
        "### Шаг 4: Градиентный спуск\n",
        "\n",
        "Для обновления параметров $ \\theta $ используется градиентный спуск:\n",
        "\n",
        "$$ \\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j} $$\n",
        "\n",
        "где $ \\alpha $ — скорость обучения (learning rate), $ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} $ — градиент функции потерь по параметру $ \\theta_j $.\n",
        "\n",
        "### Шаг 5: Предсказание\n",
        "\n",
        "После обучения модели, для нового документа $ x_{\\text{новый}} $, вероятность принадлежности к классу \"спам\" вычисляется как:\n",
        "\n",
        "$$ P(y = 1 | x_{\\text{новый}}) = \\sigma(\\theta^T x_{\\text{новый}}) $$\n",
        "\n",
        "Если $ P(y = 1 | x_{\\text{новый}}) > 0.5 $, предсказываем, что документ является спамом; если $ P(y = 1 | x_{\\text{новый}}) \\leq 0.5 $, предсказываем, что документ не является спамом.\n",
        "\n",
        "### Пример расчета\n",
        "\n",
        "Предположим, у нас есть новый документ: \"купить качество\". Мы вычисляем его вектор признаков $ x_{\\text{новый}} = [1, 0, 1, 1, 0, 0] $.\n",
        "\n",
        "Чтобы классифицировать его:\n",
        "\n",
        "$$ \\theta = [-1, 0.5, 1, 1, -0.5, 0] $$ (примерный вектор параметров, не реальные числа)\n",
        "\n",
        "$$ \\theta^T x_{\\text{новый}} = -1 \\cdot 1 + 0.5 \\cdot 0 + 1 \\cdot 1 + 1 \\cdot 1 + (-0.5) \\cdot 0 + 0 \\cdot 0 = 1.5 $$\n",
        "\n",
        "Теперь вычисляем вероятность принадлежности к классу \"спам\":\n",
        "\n",
        "$$ P(y = 1 | x_{\\text{новый}}) = \\sigma(1.5) = \\frac{1}{1 + e^{-1.5}} \\approx 0.817 $$\n",
        "\n",
        "Так как $ P(y = 1 | x_{\\text{новый}}) > 0.5 $, предсказываем, что документ \"купить качество\" является спамом.\n",
        "\n",
        "Это основы решения задачи классификации текста с использованием логистической регрессии.\n",
        "\n"
      ],
      "metadata": {
        "id": "oDMunibpqgiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализация на питон"
      ],
      "metadata": {
        "id": "vJ99HbWCxoQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Пример данных\n",
        "data = {\n",
        "    'text': ['купить дешево', 'хорошее качество', 'купить быстро дешево', 'качество гарантировано',\n",
        "             'дешево и быстро', 'отличное качество', 'купить сразу', 'качество на высоте'],\n",
        "    'label': ['спам', 'не спам', 'спам', 'не спам', 'спам', 'не спам', 'спам', 'не спам']\n",
        "}\n",
        "\n",
        "# Создаем DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Разделение на обучающую и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.25, random_state=42)\n",
        "\n",
        "# Преобразование текста в вектор признаков\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Создание и обучение модели логистической регрессии\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# Предсказание на тестовой выборке\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "# Оценка точности\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "# Подробный отчет о классификации\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPE3JeM4wGhE",
        "outputId": "a11d314f-ff5c-4c7e-9d61-fa314193cbe1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     не спам       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           1.00         2\n",
            "   macro avg       1.00      1.00      1.00         2\n",
            "weighted avg       1.00      1.00      1.00         2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Это действительно отличный результат! Модель логистической регрессии показывает идеальную точность на данной небольшой тестовой выборке.\n",
        "\n",
        "Давайте разберем полученные метрики:\n",
        "\n",
        "- **Accuracy**: 1.0 - это означает, что все предсказания модели были абсолютно верными, то есть все объекты были классифицированы правильно.\n",
        "- **Precision, Recall, F1-score**: Поскольку у нас бинарная классификация (\"спам\" и \"не спам\"), все эти метрики равны 1.0 для класса \"не спам\". Это означает, что модель не допустила ни одной ошибки при классификации объектов данного класса.\n",
        "- **Support**: Показывает, что в тестовой выборке было 2 объекта класса \"не спам\".\n",
        "\n",
        "Такой отличный результат может быть связан с тем, что:\n",
        "\n",
        "1. Данные очень хорошо разделяются по классам, и логистическая регрессия может точно выделить границу между классами.\n",
        "2. Размер обучающей и тестовой выборок достаточно мал, поэтому модель \"запоминает\" шаблоны и идеально классифицирует объекты.\n",
        "\n",
        "В реальных задачах чаще всего получить такой высокий уровень точности не удается, и нужно уделять больше внимания оценке модели на независимых данных, анализу ошибок, подбору гиперпараметров и т.д. Но этот пример показывает, что логистическая регрессия является мощным инструментом для бинарной классификации."
      ],
      "metadata": {
        "id": "p5XXCkWzw2lX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для использования результатов классификации можно выполнить следующие шаги:\n",
        "\n",
        "1. Получение предсказаний: После обучения модели, вы можете использовать метод predict() для получения предсказаний на новых данных. Например:\n",
        "\n"
      ],
      "metadata": {
        "id": "uiEJ4Oy_wouJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = ['купить срочно дешево', 'качество супер']\n",
        "new_vec = vectorizer.transform(new_text)\n",
        "new_predictions = model.predict(new_vec)\n",
        "print(f'Предсказания: {new_predictions}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a66623b6-c584-417e-87c0-8892fc1903db",
        "id": "jaY_6h-8wouL"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предсказания: ['спам' 'не спам']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Принятие решений**:\n",
        "\n",
        "Например, если мы получили предсказание 'спам', мы можем автоматически перемещать сообщение в папку со спамом:\n"
      ],
      "metadata": {
        "id": "yRUwi4qlwouO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for pred in new_predictions:\n",
        "    if pred == 'спам':\n",
        "        print('Сообщение перемещено в папку со спамом')\n",
        "    else:\n",
        "        print('Сообщение добавлено в inbox')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c2dbd1b-1f60-4252-d8c2-97bb199e4ef1",
        "id": "XWXNGHtfwouP"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сообщение перемещено в папку со спамом\n",
            "Сообщение добавлено в inbox\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SVM"
      ],
      "metadata": {
        "id": "qw17BDTPw7Lq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Введение в SVM (Support Vector Machines)\n",
        "\n",
        "**Support Vector Machines (SVM)** - это мощный алгоритм машинного обучения, который может использоваться для задач классификации, регрессии и других задач машинного обучения. В контексте обработки естественного языка (NLP), SVM часто применяется для задач классификации текстовых данных, таких как определение тональности текста, категоризация документов и других.\n",
        "\n",
        "#### Основные концепции SVM\n",
        "\n",
        "1. **Разделяющая гиперплоскость**: SVM ищет оптимальную гиперплоскость, которая лучше всего разделяет два класса объектов в многомерном пространстве признаков.\n",
        "\n",
        "2. **Опорные вектора**: Это объекты обучающего набора данных, которые находятся на границе между классами и влияют на положение разделяющей гиперплоскости.\n",
        "\n",
        "3. **Ядерные функции**: SVM может использовать различные ядерные функции для преобразования признакового пространства, что позволяет лучше разделять классы в сложных пространствах признаков.\n",
        "\n",
        "#### SVM для NLP\n",
        "\n",
        "В контексте NLP, SVM часто используется для задач классификации текста, таких как:\n",
        "\n",
        "- **Определение тональности текста** (позитивный, негативный, нейтральный).\n",
        "- **Классификация документов** (например, новостные статьи по теме).\n",
        "- **Определение категории текста** (спорт, политика, технологии и т.д.).\n",
        "\n",
        "#### Пример работы SVM для задачи классификации тональности текста\n",
        "\n",
        "Давайте рассмотрим пример применения SVM для задачи определения тональности текста. Предположим, у нас есть набор данных с отзывами о продуктах, и мы хотим определить, положительный ли отзыв или отрицательный на основе текста.\n",
        "\n",
        "##### Шаги:\n",
        "\n",
        "1. **Предобработка данных**: Преобразование текста в векторы признаков (например, с помощью метода TF-IDF или Word2Vec).\n",
        "\n",
        "2. **Построение SVM модели**:\n",
        "\n",
        "   - Пусть у нас есть обучающий набор данных $ D = \\{(x_i, y_i)\\}_{i=1}^{n} $, где $ x_i $ - вектор признаков для i-го текста, $ y_i \\in \\{-1, +1\\} $ - метка класса (негативный или положительный отзыв).\n",
        "   \n",
        "   - SVM ищет гиперплоскость в форме $ \\mathbf{w} \\cdot \\mathbf{x} + b = 0 $, которая разделяет два класса, где $ \\mathbf{w} $ - веса признаков, $ b $ - смещение (bias).\n",
        "\n",
        "   - Целевая функция SVM для разделяющей гиперплоскости:\n",
        "     $$ \\min_{\\mathbf{w}, b} \\frac{1}{2} \\| \\mathbf{w} \\|^2 + C \\sum_{i=1}^{n} \\max(0, 1 - y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b)) $$\n",
        "     где $ C $ - параметр регуляризации.\n",
        "\n",
        "   - SVM использует ядерные функции для перехода в более высокие пространства признаков, например, полиномиальное ядро $ K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i \\cdot \\mathbf{x}_j + c)^d $ или радиальное базисное функциональное ядро (RBF).\n",
        "\n",
        "3. **Оценка модели**: После обучения модели SVM используется тестовый набор данных для оценки её качества. Можно использовать различные метрики, такие как точность, полнота, F1-мера и ROC-кривая.\n",
        "\n",
        "#### Метрики оценки модели SVM для NLP\n",
        "\n",
        "- **Точность (Accuracy)**: $ \\frac{TP + TN}{TP + TN + FP + FN} $, где $ TP $, $ TN $, $ FP $, $ FN $ - true positive, true negative, false positive, false negative соответственно.\n",
        "\n",
        "- **Полнота (Recall)**: $ \\frac{TP}{TP + FN} $, где $ TP $ - true positive, $ FN $ - false negative.\n",
        "\n",
        "- **Точность (Precision)**: $ \\frac{TP}{TP + FP} $, где $ FP $ - false positive.\n",
        "\n",
        "- **F1-мера (F1-score)**: $ 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} $, гармоническое среднее точности и полноты.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3x96A-pTx_KW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Пример.**\n",
        "Рассмотрим задачу классификации текстов на примере модели SVM. У нас есть обучающая выборка из четырех документов, которые мы хотим классифицировать как спам или не спам.\n",
        "\n",
        "**Обучающая выборка:**\n",
        "\n",
        "- Документ 1 (спам): \"купить дешево\"\n",
        "- Документ 2 (не спам): \"хорошее качество\"\n",
        "- Документ 3 (спам): \"купить быстро дешево\"\n",
        "- Документ 4 (не спам): \"качество гарантировано\"\n",
        "\n",
        "**Шаг 1: Представление данных**\n",
        "\n",
        "Каждый документ представляется в виде вектора признаков с использованием мешка слов на основе словаря:\n",
        "\n",
        "- Документ 1 (спам): \\( x^{(1)} = [1, 1, 0, 0, 0, 0] \\)\n",
        "- Документ 2 (не спам): \\( x^{(2)} = [0, 0, 1, 1, 0, 0] \\)\n",
        "- Документ 3 (спам): \\( x^{(3)} = [1, 1, 0, 0, 1, 0] \\)\n",
        "- Документ 4 (не спам): \\( x^{(4)} = [0, 0, 0, 1, 0, 1] \\)\n",
        "\n",
        "**Шаг 2: Формулирование модели SVM**\n",
        "\n",
        "Модель SVM строит разделяющую гиперплоскость в многомерном пространстве признаков для классификации.\n",
        "\n",
        "### Шаг 3: Функция решающего правила\n",
        "\n",
        "Цель SVM - найти оптимальную разделяющую гиперплоскость, максимизирующую зазор между классами. Для классификации нового документа \\( x_{\\text{новый}} \\), используем:\n",
        "\n",
        "$$ f(x_{\\text{новый}}) = \\text{sign}(\\theta^T x_{\\text{новый}} + b) $$\n",
        "\n",
        "где \\( \\theta \\) — вектор весов, \\( b \\) — смещение (bias), \\( \\text{sign} \\) — функция знака.\n",
        "\n",
        "### Пример расчета\n",
        "\n",
        "Предположим, у нас есть новый документ \"купить качество\". Его вектор признаков: \\( x_{\\text{новый}} = [1, 0, 1, 1, 0, 0] \\).\n",
        "\n",
        "Чтобы классифицировать его, мы используем полученную модель SVM:\n",
        "\n",
        "$$ \\theta = [1, 0, -1, 1, -0.5, 0] $$ (примерный вектор параметров, не реальные числа)\n",
        "\n",
        "$$ b = 0 $$ (примерное смещение)\n",
        "\n",
        "Вычисляем решающее правило:\n",
        "\n",
        "$$ f(x_{\\text{новый}}) = \\text{sign}(1 \\cdot 1 + 0 \\cdot 0 + (-1) \\cdot 1 + 1 \\cdot 1 + (-0.5) \\cdot 0 + 0 \\cdot 0 + 0) $$\n",
        "\n",
        "$$ f(x_{\\text{новый}}) = \\text{sign}(1) = 1 $$\n",
        "\n",
        "Так как результат \\( f(x_{\\text{новый}}) > 0 \\), мы предсказываем, что документ \"купить качество\" относится к классу спама.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "L-S_Q9nh4jKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Таким образом, Support Vector Machines (SVM) представляют собой мощный инструмент для классификации текстовых данных в NLP благодаря своей способности строить эффективные разделяющие гиперплоскости в многомерных пространствах признаков. Важно учитывать выбор ядра и настройку параметров модели для достижения оптимальных результатов."
      ],
      "metadata": {
        "id": "SixIbZYT4jW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Пример данных\n",
        "data = {\n",
        "    'text': ['купить дешево', 'хорошее качество', 'купить быстро дешево', 'качество гарантировано',\n",
        "             'дешево и быстро', 'отличное качество', 'купить сразу', 'качество на высоте'],\n",
        "    'label': ['спам', 'не спам', 'спам', 'не спам', 'спам', 'не спам', 'спам', 'не спам']\n",
        "}\n",
        "\n",
        "# Создаем DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Разделение на обучающую и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.25, random_state=42)\n",
        "\n",
        "# Преобразование текста в вектор признаков\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Создание и обучение модели SVC\n",
        "model = SVC()\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# Предсказание на тестовой выборке\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "# Оценка точности\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "# Подробный отчет о классификации\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20529aa5-addc-45de-8976-29e7973b714f",
        "id": "jhXe_dld56jk"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     не спам       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           1.00         2\n",
            "   macro avg       1.00      1.00      1.00         2\n",
            "weighted avg       1.00      1.00      1.00         2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Это отличный результат! Модель демонстрирует 100% точность на тестовой выборке. Это значит, что она безошибочно классифицирует сообщения как \"спам\" или \"не спам\".\n",
        "\n",
        "Такой высокий уровень производительности на небольшом наборе данных может свидетельствовать о том, что модель хорошо обучилась на этих примерах, однако важно проверить, как она будет работать на более разнообразных и реалистичных данных. Тем не менее, на данном этапе можно сделать следующие выводы:\n",
        "\n",
        "1. Модель SVM успешно справилась с задачей классификации сообщений на \"спам\" и \"не спам\".\n",
        "2. Показатели точности, полноты и F1-меры равны 1.0, что говорит о высоком качестве классификации.\n",
        "3. Модель может быть использована для автоматической фильтрации спама в реальном приложении.\n",
        "\n",
        "Дальнейшие шаги могут включать в себя:\n",
        "\n",
        "- Тестирование модели на более крупных и разнообразных наборах данных, чтобы оценить ее обобщающую способность.\n",
        "- Настройку гиперпараметров модели SVM для улучшения ее производительности.\n",
        "- Сравнение производительности SVM с другими алгоритмами машинного обучения, такими как логистическая регрессия, наивный байесовский классификатор и т.д.\n",
        "- Интеграцию модели в реальное приложение для фильтрации спама.\n",
        "\n",
        "В целом, это очень хороший результат, который демонстрирует эффективность применения SVM для задачи классификации текстовых данных."
      ],
      "metadata": {
        "id": "LrMNsM8RBAqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для использования результатов классификации можно выполнить следующие шаги:\n",
        "\n",
        "1. Получение предсказаний: После обучения модели, вы можете использовать метод predict() для получения предсказаний на новых данных. Например:\n",
        "\n"
      ],
      "metadata": {
        "id": "kL-b0zze56jn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = ['купить срочно дешево', 'качество супер']\n",
        "new_vec = vectorizer.transform(new_text)\n",
        "new_predictions = model.predict(new_vec)\n",
        "print(f'Предсказания: {new_predictions}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4f9b7fe-3dca-40e6-fe98-5baa055018ab",
        "id": "Z1VPeeE056jo"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предсказания: ['спам' 'не спам']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Принятие решений**:\n",
        "\n",
        "Например, если мы получили предсказание 'спам', мы можем автоматически перемещать сообщение в папку со спамом:\n"
      ],
      "metadata": {
        "id": "uT41Bg-356jp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for pred in new_predictions:\n",
        "    if pred == 'спам':\n",
        "        print('Сообщение перемещено в папку со спамом')\n",
        "    else:\n",
        "        print('Сообщение добавлено в inbox')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53cf3cb6-1ef0-4e8f-ef71-d99dc5ec9569",
        "id": "XGwtcYB_56jq"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сообщение перемещено в папку со спамом\n",
            "Сообщение добавлено в inbox\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **K-ближайших соседей (K-Nearest Neighbors, KNN)**"
      ],
      "metadata": {
        "id": "QYEGYg2-BDf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Введение в метод KNN\n",
        "\n",
        "Метод KNN является одним из простейших и понятных алгоритмов машинного обучения. Он используется как для задач классификации, так и для задач регрессии. Основная идея метода заключается в том, чтобы классифицировать (или предсказать) новые точки данных на основе их близости к уже известным точкам данных.\n",
        "\n",
        "#### Основные принципы KNN\n",
        "\n",
        "1. **Классификация**: Для классификации новой точки данных определяется её класс на основе классов её ближайших соседей.\n",
        "2. **Регрессия**: Для регрессии новая точка данных получает предсказанное значение на основе значений её ближайших соседей.\n",
        "\n",
        "#### Алгоритм KNN\n",
        "\n",
        "1. **Шаг 1**: Загрузка обучающих данных.\n",
        "2. **Шаг 2**: Выбор числа ближайших соседей $ k $.\n",
        "3. **Шаг 3**: Определение метрики расстояния (например, евклидово расстояние).\n",
        "4. **Шаг 4**: Для новой точки данных вычисление расстояний до всех точек обучающего набора.\n",
        "5. **Шаг 5**: Отбор $ k $ ближайших точек.\n",
        "6. **Шаг 6**: Определение класса (или значения) новой точки данных на основе классов (или значений) её ближайших соседей.\n",
        "\n",
        "#### Метрики расстояния\n",
        "\n",
        "1. **Евклидово расстояние**: Для двух точек $ \\mathbf{p} = (p_1, p_2, \\ldots, p_n) $ и $ \\mathbf{q} = (q_1, q_2, \\ldots, q_n) $:\n",
        "   $$\n",
        "   \\text{EuclideanDistance}(\\mathbf{p}, \\mathbf{q}) = \\sqrt{\\sum_{i=1}^{n} (p_i - q_i)^2}\n",
        "   $$\n",
        "\n",
        "2. **Манхэттенское расстояние**: Для двух точек $ \\mathbf{p} = (p_1, p_2, \\ldots, p_n) $ и $ \\mathbf{q} = (q_1, q_2, \\ldots, q_n) $:\n",
        "   $$\n",
        "   \\text{ManhattanDistance}(\\mathbf{p}, \\mathbf{q}) = \\sum_{i=1}^{n} |p_i - q_i|\n",
        "   $$\n",
        "\n",
        "#### Оценка качества модели\n",
        "\n",
        "Для оценки качества модели KNN важно использовать метрики, такие как:\n",
        "\n",
        "1. **Accuracy (точность)**: Доля правильно классифицированных объектов.\n",
        "   $$\n",
        "   \\text{Accuracy} = \\frac{\\text{Количество правильных предсказаний}}{\\text{Общее количество предсказаний}}\n",
        "   $$\n",
        "\n",
        "2. **Precision (точность)**: Доля верно предсказанных положительных классов среди всех предсказанных положительных классов.\n",
        "   $$\n",
        "   \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
        "   $$\n",
        "\n",
        "3. **Recall (полнота)**: Доля верно предсказанных положительных классов среди всех реальных положительных классов.\n",
        "   $$\n",
        "   \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
        "   $$\n",
        "\n"
      ],
      "metadata": {
        "id": "Xiks9IRSCdLP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Пример\n",
        "\n",
        "Рассмотрим простой пример классификации текста на спам и не спам.\n",
        "\n",
        "**Обучающая выборка:**\n",
        "\n",
        "- Документ 1 (спам): \"купить дешево\"\n",
        "- Документ 2 (не спам): \"хорошее качество\"\n",
        "- Документ 3 (спам): \"купить быстро дешево\"\n",
        "- Документ 4 (не спам): \"качество гарантировано\"\n",
        "\n",
        "Словарь: \"купить\", \"дешево\", \"хорошее\", \"качество\", \"быстро\", \"гарантировано\".\n",
        "тность делают его популярным в NLP.\n",
        "\n",
        "### Пример классификации текста с использованием KNN\n",
        "\n",
        "#### Обучающая выборка:\n",
        "\n",
        "Представим, что каждый текст представлен вектором TF-IDF (Term Frequency-Inverse Document Frequency) на основе словаря из шести слов: \"купить\", \"дешево\", \"хорошее\", \"качество\", \"быстро\", \"гарантировано\". Давайте представим обучающую выборку в виде векторов TF-IDF.\n",
        "\n",
        "- Документ 1 (спам): \"купить дешево\"  \n",
        "  TF-IDF вектор: $ [1, 1, 0, 0, 0, 0] $\n",
        "\n",
        "- Документ 2 (не спам): \"хорошее качество\"  \n",
        "  TF-IDF вектор: $ [0, 0, 1, 1, 0, 0] $\n",
        "\n",
        "- Документ 3 (спам): \"купить быстро дешево\"  \n",
        "  TF-IDF вектор: $ [1, 1, 0, 0, 1, 0] $\n",
        "\n",
        "- Документ 4 (не спам): \"качество гарантировано\"  \n",
        "  TF-IDF вектор: $ [0, 0, 0, 1, 0, 1] $\n",
        "\n",
        "#### Классификация нового документа \"купить качество\":\n",
        "\n",
        "1. **Вычисление расстояний до всех обучающих примеров** (используем евклидово расстояние в пространстве TF-IDF):\n",
        "\n",
        "   - Документ \"купить качество\" TF-IDF вектор: $ [1, 0, 0, 1, 0, 0] $\n",
        "\n",
        "   Расстояния:\n",
        "   - До Документа 1 (спам): $ \\sqrt{(1-1)^2 + (1-0)^2 + (0-0)^2 + (0-1)^2 + (0-0)^2 + (0-0)^2} = \\sqrt{1} = 1 $\n",
        "   - До Документа 2 (не спам): $ \\sqrt{(0-1)^2 + (0-0)^2 + (1-0)^2 + (1-1)^2 + (0-0)^2 + (0-0)^2} = \\sqrt{2} \\approx 1.414 $\n",
        "   - До Документа 3 (спам): $ \\sqrt{(1-1)^2 + (1-0)^2 + (0-0)^2 + (0-1)^2 + (1-0)^2 + (0-0)^2} = \\sqrt{1} = 1 $\n",
        "   - До Документа 4 (не спам): $ \\sqrt{(0-1)^2 + (0-0)^2 + (0-0)^2 + (1-1)^2 + (0-0)^2 + (0-0)^2} = \\sqrt{1} = 1 $\n",
        "\n",
        "2. **Выбор $ k $ ближайших соседей**:\n",
        "\n",
        "   Пусть $ k = 3 $. Ближайшие документы по расстоянию: Документ 1 (спам), Документ 3 (спам), Документ 4 (не спам).\n",
        "\n",
        "3. **Определение класса нового документа**:\n",
        "\n",
        "   - Документ 1 (спам)\n",
        "   - Документ 3 (спам)\n",
        "   - Документ 4 (не спам)\n",
        "\n",
        "   Исходя из большинства среди $ k $ ближайших соседей, новый документ \"купить качество\" будет классифицирован как \"спам\", так как 2 из 3 ближайших соседей относятся к классу \"спам\".\n",
        "\n",
        "#### Заключение\n",
        "\n",
        "Метод KNN представляет собой простой и эффективный алгоритм для классификации текстов на основе их схожести с обучающими примерами. Он особенно полезен в NLP благодаря простоте векторизации текстовых данных и непосредственной работы с метриками сходства.\n"
      ],
      "metadata": {
        "id": "FjR_c5epGai4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализация на питон"
      ],
      "metadata": {
        "id": "sJwmX04BHN1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Пример данных\n",
        "data = {\n",
        "    'text': ['купить дешево', 'хорошее качество', 'купить быстро дешево', 'качество гарантировано',\n",
        "             'дешево и быстро', 'отличное качество', 'купить сразу', 'качество на высоте'],\n",
        "    'label': ['спам', 'не спам', 'спам', 'не спам', 'спам', 'не спам', 'спам', 'не спам']\n",
        "}\n",
        "\n",
        "# Создаем DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Разделение на обучающую и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.25, random_state=42)\n",
        "\n",
        "# Преобразование текста в вектор признаков\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Создание и обучение модели KNN\n",
        "model = KNeighborsClassifier()\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# Предсказание на тестовой выборке\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "# Оценка точности\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "# Подробный отчет о классификации\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50eb3e2c-3263-4c01-8412-59718684fe83",
        "id": "2cLeu9FaHeWs"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     не спам       0.00      0.00      0.00       2.0\n",
            "        спам       0.00      0.00      0.00       0.0\n",
            "\n",
            "    accuracy                           0.00       2.0\n",
            "   macro avg       0.00      0.00      0.00       2.0\n",
            "weighted avg       0.00      0.00      0.00       2.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ошибка с нулевой точностью и пустым отчетом о классификации указывает на проблему в данных или модели.\n",
        "\n",
        "Давайте оптимизируем и упростим код, обеспечив корректное разделение данных и добавив проверку для сбалансированности классов в обучающей и тестовой выборках.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Оптимизированный код"
      ],
      "metadata": {
        "id": "U9SrITPzJw_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Пример данных\n",
        "data = {\n",
        "    'text': ['купить дешево', 'хорошее качество', 'купить быстро дешево', 'качество гарантировано',\n",
        "             'дешево и быстро', 'отличное качество', 'купить сразу', 'качество на высоте'],\n",
        "    'label': ['спам', 'не спам', 'спам', 'не спам', 'спам', 'не спам', 'спам', 'не спам']\n",
        "}\n",
        "\n",
        "# Создаем DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Балансировка данных с помощью ресемплинга\n",
        "df_majority = df[df.label == 'не спам']\n",
        "df_minority = df[df.label == 'спам']\n",
        "\n",
        "# Увеличение меньшинства с ресемплингом\n",
        "df_minority_upsampled = resample(df_minority,\n",
        "                                 replace=True,     # Увеличиваем с повторением\n",
        "                                 n_samples=len(df_majority),    # Увеличиваем до числа в классе \"не спам\"\n",
        "                                 random_state=42)\n",
        "\n",
        "# Комбинируем обратно сбалансированные данные\n",
        "df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
        "\n",
        "# Разделение на обучающую и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_balanced['text'], df_balanced['label'], test_size=0.25, random_state=42)\n",
        "\n",
        "# Преобразование текста в вектор признаков\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Создание и обучение модели KNN\n",
        "model = KNeighborsClassifier(n_neighbors=3)  # Выбираем k=3 для этого примера\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# Предсказание на тестовой выборке\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "# Оценка точности\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "# Подробный отчет о классификации\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSwvcKDwJ_Z_",
        "outputId": "1afaea7b-e770-4127-8412-4bcbc56d50c1"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     не спам       1.00      1.00      1.00         1\n",
            "        спам       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           1.00         2\n",
            "   macro avg       1.00      1.00      1.00         2\n",
            "weighted avg       1.00      1.00      1.00         2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Объяснение изменений\n",
        "\n",
        "1. **Балансировка данных**: Добавлен шаг балансировки данных с помощью ресемплинга, чтобы гарантировать, что обучающая выборка содержит одинаковое количество примеров каждого класса. Это помогает избежать проблем с несбалансированными классами, что может привести к нулевой точности.\n",
        "   \n",
        "2. **Оптимизация импорта и использования sklearn**: Используются наиболее подходящие методы для балансировки данных и разделения их на обучающую и тестовую выборки.\n",
        "\n",
        "3. **Настройка модели KNN**: Задали параметр `n_neighbors=3` для модели KNN, чтобы продемонстрировать выбор числа ближайших соседей.\n",
        "\n",
        "### Примечание\n",
        "\n",
        "Если данные в реальных сценариях остаются несбалансированными, можно использовать дополнительные методы, такие как взвешивание классов или специальные метрики оценки, которые лучше справляются с несбалансированными наборами данных."
      ],
      "metadata": {
        "id": "WJzv8LvYK5Qa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Основываясь на предоставленных результатах, где точность модели составляет 1.0 (100%), а отчет о классификации показывает идеальные результаты (precision, recall и f1-score равны 1.0 для обоих классов), можно сделать вывод, что модель KNN отлично справилась с классификацией текстовых данных на \"спам\" и \"не спам\" в этом конкретном примере.\n",
        "\n",
        "Ключевые моменты:\n",
        "\n",
        "1. **Точность (Accuracy)**: Модель правильно классифицировала все примеры в тестовой выборке, что говорит о ее высокой производительности.\n",
        "\n",
        "2. **Отчет о классификации (Classification Report)**:\n",
        "   - Precision (точность): 1.0 для обоих классов, что означает, что модель не допускает ложноположительных срабатываний.\n",
        "   - Recall (полнота): 1.0 для обоих классов, что указывает на то, что модель не пропускает ни одного примера.\n",
        "   - F1-score (гармоническое среднее precision и recall): 1.0 для обоих классов, что является идеальным результатом.\n",
        "\n",
        "Такие высокие показатели на небольшом тестовом наборе данных свидетельствуют о том, что модель KNN отлично справляется с данной задачей классификации текста на \"спам\" и \"не спам\". Однако для более объективной оценки модели необходимо протестировать ее на более обширных и разнообразных данных."
      ],
      "metadata": {
        "id": "zeLbWMU_Kx-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для использования результатов классификации можно выполнить следующие шаги:\n",
        "\n",
        "1. Получение предсказаний: После обучения модели, вы можете использовать метод predict() для получения предсказаний на новых данных. Например:\n",
        "\n"
      ],
      "metadata": {
        "id": "pZwwxkYLHeWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = ['купить срочно дешево', 'качество супер']\n",
        "new_vec = vectorizer.transform(new_text)\n",
        "new_predictions = model.predict(new_vec)\n",
        "print(f'Предсказания: {new_predictions}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70149ff5-36de-4151-da7f-5a5ffcf8d750",
        "id": "-rQDGyLUHeWu"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предсказания: ['спам' 'не спам']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Принятие решений**:\n",
        "\n",
        "Например, если мы получили предсказание 'спам', мы можем автоматически перемещать сообщение в папку со спамом:\n"
      ],
      "metadata": {
        "id": "DdYH0943HeWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for pred in new_predictions:\n",
        "    if pred == 'спам':\n",
        "        print('Сообщение перемещено в папку со спамом')\n",
        "    else:\n",
        "        print('Сообщение добавлено в inbox')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81a32def-455a-4446-8d12-6fc42865904a",
        "id": "SVv6zeTuHeWw"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сообщение перемещено в папку со спамом\n",
            "Сообщение добавлено в inbox\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Деревья решений (Decision Trees)**"
      ],
      "metadata": {
        "id": "My8TRNm2CdPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cRXV0W2tCdmf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Случайный лес (Random Forest)**\n"
      ],
      "metadata": {
        "id": "ybsWE6kRCdpt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6ObfXOm1CrKQ"
      }
    }
  ]
}